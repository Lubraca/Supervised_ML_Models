{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbb851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Project configuration (Paths) initialized successfully.\n",
      "Raw Data Path check: /content/drive/MyDrive/Project_01/data/raw/application_train.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Encoded Train Data. Shape: (307511, 135)\n",
      "✅ Loaded Encoded Test Data. Shape: (48744, 134)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CORE LIBRARIES AND UTILITIES ---\n",
    "# Standard data manipulation and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# --- 2. SCIKIT-LEARN: MODEL SELECTION & METRICS ---\n",
    "# Tools for splitting data, cross-validation, and performance evaluation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    KFold, \n",
    "    cross_val_score, \n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline # For creating robust modeling workflows\n",
    "from sklearn.impute import SimpleImputer # Tool to handle NaNs\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Define project Path in Colab\n",
    "PROJECT_BASE_PATH = '/content/drive/MyDrive/Project_01' \n",
    "\n",
    "# ADD 'src' DIRECTORY TO PYTHON PATH\n",
    "SRC_PATH = os.path.join(PROJECT_BASE_PATH, 'src')\n",
    "\n",
    "# verify if SRC_PATH is already in sys.path\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "    print(\"✅ Successfully added 'src' directory to Python path.\")\n",
    "\n",
    "# IMPORT Paths CLASS FROM config MODULE\n",
    "from config import Paths\n",
    "\n",
    "try:\n",
    "    from config import Paths\n",
    "    \n",
    "    # 3. Inicialize a instância com um nome único (cfg)\n",
    "    cfg = Paths(PROJECT_BASE_PATH) # <-- Mudança aqui\n",
    "    cfg.create_dirs() \n",
    "    \n",
    "    print(\"\\n✅ Project configuration (Paths) initialized successfully.\")\n",
    "    print(f\"Raw Data Path check: {cfg.TRAIN_RAW_FILE}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"❌ Error: Could not import Paths from config module.\")\n",
    "\n",
    "FINAL_TRAIN_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'train_final_encoded.parquet')\n",
    "FINAL_TEST_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'test_final_encoded.parquet')\n",
    "\n",
    "try:\n",
    "    # Read the Parquet files\n",
    "    df_train_final = pd.read_parquet(FINAL_TRAIN_FILE)\n",
    "    df_test_final = pd.read_parquet(FINAL_TEST_FILE)\n",
    "    \n",
    "    print(f\"✅ Loaded Encoded Train Data. Shape: {df_train_final.shape}\")\n",
    "    print(f\"✅ Loaded Encoded Test Data. Shape: {df_test_final.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ ERROR: Parquet files not found. Ensure that the parquet was executed successfully in Feature_Eng notebook and saved the files.\")\n",
    "    # Exit or raise an error if critical data is missing\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17648dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting: Final Model Training for Deployment ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature matrices prepared and cleaned. Training shape: (307511, 133)\n",
      "--- Training final model using optimized parameters: {'n_estimators': 1000, 'learning_rate': 0.01, 'num_leaves': 63, 'metric': 'auc', 'boosting_type': 'gbdt', 'n_jobs': -1, 'seed': 42, 'verbose': -1} ---\n"
     ]
    }
   ],
   "source": [
    "# Final LightGBM Training with Optimized Hyperparameters\n",
    "\n",
    "# Imports are omitted as requested, assuming lgb, time, pd, joblib, and necessary configuration (cfg) are imported.\n",
    "\n",
    "print(\"--- Starting: Final Model Training for Deployment ---\")\n",
    "\n",
    "# 1. Prepare Features (X) and Target (y)\n",
    "EXCLUDED_COLS = ['SK_ID_CURR', 'TARGET'] \n",
    "features = [col for col in df_train_final.columns if col not in EXCLUDED_COLS]\n",
    "\n",
    "X = df_train_final[features]\n",
    "y = df_train_final['TARGET']\n",
    "\n",
    "X_test = df_test_final[features]\n",
    "\n",
    "# Final NaN/Inf Handling (Crucial)\n",
    "# This step ensures X is fully numerical after all previous cleaning steps.\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(X.mean())\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_test.mean())\n",
    "print(f\"✅ Feature matrices prepared and cleaned. Training shape: {X.shape}\")\n",
    "\n",
    "\n",
    "# 2. Configure Final Model using Optimized Parameters\n",
    "# NOTE: Replace these placeholder values with the ACTUAL best parameters \n",
    "# found in your '04_Model_Tuning_GridSearch' notebook.\n",
    "OPTIMAL_PARAMS = {\n",
    "    'n_estimators': 1000,   \n",
    "    'learning_rate': 0.01, \n",
    "    'num_leaves': 63,       \n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "final_model = lgb.LGBMClassifier(**OPTIMAL_PARAMS)\n",
    "\n",
    "print(f\"--- Training final model using optimized parameters: {OPTIMAL_PARAMS} ---\")\n",
    "\n",
    "# 3. Final Training on FULL Dataset\n",
    "start_time = time()\n",
    "final_model.fit(X, y)\n",
    "end_time = time()\n",
    "\n",
    "print(f\"✅ Final training completed in {(end_time - start_time):.2f} seconds.\")\n",
    "\n",
    "\n",
    "# 4. MLOps Artifact 1: Save the Optimized Model \n",
    "# This file is the core component for your API endpoint (e.g., FastAPI/Flask).\n",
    "model_file_path = os.path.join (cfg.MODEL_DIR, 'final_lgbm_optimized_model.pkl')\n",
    "\n",
    "joblib.dump(final_model, model_file_path)\n",
    "\n",
    "print(f\"✅ MLOps Artifact: Optimized Model saved to: {model_file_path.name}\")\n",
    "\n",
    "\n",
    "# 5. MLOps Artifact 2: Submission File Creation \n",
    "submission_ids = df_test_final['SK_ID_CURR'].astype(int)\n",
    "\n",
    "df_submission = pd.DataFrame({\n",
    "    'SK_ID_CURR': submission_ids,\n",
    "    'TARGET': final_model.predict_proba(X_test)[:, 1]\n",
    "})\n",
    "\n",
    "submission_file_path = os.path.join(cfg.SUBMISSION_DIR,  'submission_lgbm_final.csv')\n",
    "\n",
    "df_submission.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(\"\\n=======================================================\")\n",
    "print(f\"✅ MLOps Artifact: Final Submission file saved successfully!\")\n",
    "print(f\"File: {submission_file_path.name}\")\n",
    "print(\"=======================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
