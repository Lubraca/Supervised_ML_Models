{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafdeb7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29d0b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define project Path in Colab\n",
    "PROJECT_BASE_PATH = '/content/drive/MyDrive/Project_01' \n",
    "\n",
    "# ADD 'src' DIRECTORY TO PYTHON PATH\n",
    "SRC_PATH = os.path.join(PROJECT_BASE_PATH, 'src')\n",
    "\n",
    "# verify if SRC_PATH is already in sys.path\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "    print(\"âœ… Successfully added 'src' directory to Python path.\")\n",
    "\n",
    "# IMPORT Paths CLASS FROM config MODULE\n",
    "from config import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8182dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Project configuration (Paths) initialized successfully.\n",
      "Raw Data Path check: /content/drive/MyDrive/Project_01/data/raw/application_train.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from config import Paths\n",
    "    \n",
    "    # 3. Inicialize a instÃ¢ncia com um nome Ãºnico (cfg)\n",
    "    cfg = Paths(PROJECT_BASE_PATH) # <-- MudanÃ§a aqui\n",
    "    cfg.create_dirs() \n",
    "    \n",
    "    print(\"\\nâœ… Project configuration (Paths) initialized successfully.\")\n",
    "    print(f\"Raw Data Path check: {cfg.TRAIN_RAW_FILE}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âŒ Error: Could not import Paths from config module.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PROCESSED = os.path.join(PROJECT_BASE_PATH, 'data', 'processed')\n",
    "TRAIN_PROCESSED_FILE = os.path.join(DATA_DIR_PROCESSED, 'train_enriched.csv')\n",
    "TEST_PROCESSED_FILE = os.path.join(DATA_DIR_PROCESSED, 'test_enriched.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db1350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded final training data. Shape: (307511, 125)\n",
      "âœ… Loaded final testing data. Shape: (48744, 124)\n",
      "--- Starting Feature Engineering ---\n",
      "âœ… TRAIN: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\n",
      "âœ… TRAIN: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\n",
      "\n",
      "Training set shape after Macro Feature Engineering: (307511, 133)\n",
      "âœ… TEST: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\n",
      "âœ… TEST: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\n",
      "\n",
      "Testing set shape after Macro Feature Engineering: (48744, 132)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # --- LOAD TRAIN DATA USING CONFIG PATHS ---\n",
    "    df_train_final = pd.read_csv(cfg.TRAIN_PROCESSED_FILE)\n",
    "    \n",
    "    # ESSENTIAL FIX: Ensures TIME_INDEX is of temporal type after loading from CSV\n",
    "    df_train_final['TIME_INDEX'] = pd.to_datetime(df_train_final['TIME_INDEX'])\n",
    "    \n",
    "    print(f\"âœ… Loaded final training data. Shape: {df_train_final.shape}\")\n",
    "\n",
    "    # --- LOAD TEST DATA USING CONFIG PATHS ---\n",
    "    df_test_final = pd.read_csv(cfg.TEST_PROCESSED_FILE)\n",
    "    \n",
    "    # Apply the same temporal conversion to the Test Set\n",
    "    df_test_final['TIME_INDEX'] = pd.to_datetime(df_test_final['TIME_INDEX'])\n",
    "    \n",
    "    print(f\"âœ… Loaded final testing data. Shape: {df_test_final.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: Final processed file not found. Check if Block 9 was run and files exist at: {path.DATA_PROCESSED_DIR}\")\n",
    "    df_train_final = None\n",
    "    df_test_final = None\n",
    "if df_train_final is not None and df_test_final is not None:\n",
    "    print(\"--- Starting Feature Engineering ---\")\n",
    "    \n",
    "    # --- 1. Create Lagged and Dynamic Macro Features (TRAIN SET) ---\n",
    "    \n",
    "    # KeyError/SetIndex Fix: Ensures TIME_INDEX is a regular column before setting\n",
    "    if 'TIME_INDEX' not in df_train_final.columns and 'TIME_INDEX' in df_train_final.index.names:\n",
    "        df_train_final = df_train_final.reset_index(level='TIME_INDEX')\n",
    "        print(\"ðŸ’¡ TRAIN: TIME_INDEX restored from index to column.\")\n",
    "    \n",
    "    # Temporarily set TIME_INDEX as the index for time-series operations\n",
    "    df_train_final = df_train_final.set_index('TIME_INDEX')\n",
    "    \n",
    "    # Use the correct column names\n",
    "    macro_features_to_engineer = ['SELIC', 'IPCA'] \n",
    "    \n",
    "    for col in macro_features_to_engineer:\n",
    "        # A) Lag (Previous Month's Value)\n",
    "        df_train_final[f'{col}_LAG1'] = df_train_final.groupby('SK_ID_CURR')[col].shift(1)\n",
    "\n",
    "        # B) Change (Current Month - Previous Month)\n",
    "        df_train_final[f'{col}_CHANGE'] = df_train_final[col] - df_train_final[f'{col}_LAG1']\n",
    "        \n",
    "        # C) Trend (3-Month Rolling Mean)\n",
    "        df_train_final[f'{col}_ROLLING_MEAN3'] = df_train_final.groupby('SK_ID_CURR')[col].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        \n",
    "    print(f\"âœ… TRAIN: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\")\n",
    "\n",
    "    # --- 2. Temporal Features from TIME_INDEX (TRAIN SET) ---\n",
    "    \n",
    "    # Extract month and year using the already defined index\n",
    "    df_train_final['MONTH_OF_YEAR'] = df_train_final.index.to_series().dt.month\n",
    "    df_train_final['YEAR'] = df_train_final.index.to_series().dt.year\n",
    "    \n",
    "    print(\"âœ… TRAIN: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\")\n",
    "\n",
    "    # Restore TIME_INDEX as a regular column\n",
    "    df_train_final = df_train_final.reset_index()\n",
    "\n",
    "    print(f\"\\nTraining set shape after Macro Feature Engineering: {df_train_final.shape}\")\n",
    "    \n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --- REPEAT FOR TEST SET ---\n",
    "    # --------------------------------------------------------------------------------\n",
    "    \n",
    "    # KeyError/SetIndex Fix for the Test Set\n",
    "    if 'TIME_INDEX' not in df_test_final.columns and 'TIME_INDEX' in df_test_final.index.names:\n",
    "        df_test_final = df_test_final.reset_index(level='TIME_INDEX')\n",
    "        print(\"ðŸ’¡ TEST: TIME_INDEX restored from index to column.\")\n",
    "    \n",
    "    # Temporarily set TIME_INDEX as the index for time-series operations\n",
    "    df_test_final = df_test_final.set_index('TIME_INDEX')\n",
    "    \n",
    "    for col in macro_features_to_engineer:\n",
    "        df_test_final[f'{col}_LAG1'] = df_test_final.groupby('SK_ID_CURR')[col].shift(1)\n",
    "        df_test_final[f'{col}_CHANGE'] = df_test_final[col] - df_test_final[f'{col}_LAG1']\n",
    "        df_test_final[f'{col}_ROLLING_MEAN3'] = df_test_final.groupby('SK_ID_CURR')[col].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        \n",
    "    print(f\"âœ… TEST: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\")\n",
    "\n",
    "    # Temporal Features (TEST SET)\n",
    "    df_test_final['MONTH_OF_YEAR'] = df_test_final.index.to_series().dt.month\n",
    "    df_test_final['YEAR'] = df_test_final.index.to_series().dt.year\n",
    "    \n",
    "    print(\"âœ… TEST: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\")\n",
    "\n",
    "    # Restore TIME_INDEX as a regular column\n",
    "    df_test_final = df_test_final.reset_index()\n",
    "\n",
    "    print(f\"\\nTesting set shape after Macro Feature Engineering: {df_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f4f5c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Micro Feature Engineering ---\n",
      "âœ… Anomalous DAYS_EMPLOYED treated and flagged.\n",
      "âœ… Ratio and new temporal features created.\n",
      "Training set shape after Micro Feature Engineering: (307511, 139)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Micro Feature Engineering ---\")\n",
    "\n",
    "# Treat DAYS_EMPLOYED anomaly (365243 days â‰ˆ 1000 years) as missing (NaN) and flag it\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# --- IMPORTANT DATA CLEANING NOTE: DAYS_EMPLOYED ANOMALY ---\n",
    "# The value 365243 in DAYS_EMPLOYED is a known data anomaly specific to the Home Credit dataset.\n",
    "# It represents approximately 1000 years, and is used by the client/bank to code for\n",
    "# applicants who are currently **unemployed** or whose employment status is unverified.\n",
    "#\n",
    "# Best Practice Treatment:\n",
    "# 1. Replace the anomalous value (365243) with **NaN** to treat it as a missing value.\n",
    "# 2. Create a new **binary feature** (DAYS_EMPLOYED_ANOM) to allow the model (like LightGBM)\n",
    "#    to explicitly learn the predictive power of this specific 'unemployed/anomaly' group.\n",
    "# This prevents the model from interpreting 365243 as a literal, extremely long employment history.\n",
    "# -----------------------------------------------------------\n",
    "# --- 1. Anomaly Treatment and Transformation ---\n",
    "\n",
    "DAYS_EMPLOYED_ANOMALY_VALUE = 365243\n",
    "\n",
    "# Treat DAYS_EMPLOYED anomaly:\n",
    "# 1. Flag the anomaly first\n",
    "df_train_final['DAYS_EMPLOYED_ANOM'] = df_train_final['DAYS_EMPLOYED'] == DAYS_EMPLOYED_ANOMALY_VALUE\n",
    "df_test_final['DAYS_EMPLOYED_ANOM'] = df_test_final['DAYS_EMPLOYED'] == DAYS_EMPLOYED_ANOMALY_VALUE\n",
    "\n",
    "# 2. Replace the anomalous value with NaN using direct assignment (CORRIGIDO)\n",
    "# Remove inplace=True and assign back to the column name\n",
    "df_train_final['DAYS_EMPLOYED'] = df_train_final['DAYS_EMPLOYED'].replace({DAYS_EMPLOYED_ANOMALY_VALUE: np.nan})\n",
    "df_test_final['DAYS_EMPLOYED'] = df_test_final['DAYS_EMPLOYED'].replace({DAYS_EMPLOYED_ANOMALY_VALUE: np.nan})\n",
    "\n",
    "print(\"âœ… Anomalous DAYS_EMPLOYED treated and flagged.\")\n",
    "\n",
    "# Transform DAYS_BIRTH into AGE_YEARS\n",
    "df_train_final['AGE_YEARS'] = df_train_final['DAYS_BIRTH'] / -365\n",
    "df_test_final['AGE_YEARS'] = df_test_final['DAYS_BIRTH'] / -365\n",
    "\n",
    "# --- 2. Ratio Feature Creation ---\n",
    "\n",
    "df_train_final['INCOME_PER_AGE'] = df_train_final['AMT_INCOME_TOTAL'] / df_train_final['AGE_YEARS']\n",
    "df_test_final['INCOME_PER_AGE'] = df_test_final['AMT_INCOME_TOTAL'] / df_test_final['AGE_YEARS']\n",
    "\n",
    "df_train_final['CREDIT_INCOME_RATIO'] = df_train_final['AMT_CREDIT'] / df_train_final['AMT_INCOME_TOTAL']\n",
    "df_test_final['CREDIT_INCOME_RATIO'] = df_test_final['AMT_CREDIT'] / df_test_final['AMT_INCOME_TOTAL']\n",
    "\n",
    "df_train_final['ANNUITY_CREDIT_RATIO'] = df_train_final['AMT_ANNUITY'] / df_train_final['AMT_CREDIT']\n",
    "df_test_final['ANNUITY_CREDIT_RATIO'] = df_test_final['AMT_ANNUITY'] / df_test_final['AMT_CREDIT']\n",
    "\n",
    "df_train_final['ANNUITY_INCOME_RATIO'] = df_train_final['AMT_ANNUITY'] / df_train_final['AMT_INCOME_TOTAL']\n",
    "df_test_final['ANNUITY_INCOME_RATIO'] = df_test_final['AMT_ANNUITY'] / df_test_final['AMT_INCOME_TOTAL']\n",
    "\n",
    "print(\"âœ… Ratio and new temporal features created.\")\n",
    "print(f\"Training set shape after Micro Feature Engineering: {df_train_final.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "807ae5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Feature Encoding (Block 13) ---\n",
      "Applying Label Encoding to 3 features.\n",
      "âœ… Label Encoding completed.\n",
      "Applying One-Hot Encoding to 13 features.\n",
      "âœ… One-Hot Encoding completed.\n",
      "\n",
      "Final Train set shape: (307511, 266)\n",
      "Final Test set shape: (48744, 265)\n",
      "Total number of features after encoding: 266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"--- Starting Feature Encoding (Block 13) ---\")\n",
    "\n",
    "# --- 1. Identify Categorical Columns ---\n",
    "# Select columns that are still of type 'object'\n",
    "object_cols_train = df_train_final.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# --- 2. Label Encoding (Binary Columns: 2 Unique Categories) ---\n",
    "\n",
    "# Find columns with exactly two unique categories\n",
    "le_cols = [col for col in object_cols_train if df_train_final[col].nunique() == 2]\n",
    "print(f\"Applying Label Encoding to {len(le_cols)} features.\")\n",
    "\n",
    "for col in le_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Fit on the training set and transform both\n",
    "    df_train_final[col] = le.fit_transform(df_train_final[col])\n",
    "    \n",
    "    # Transform the test set using the same fitted encoder\n",
    "    df_test_final[col] = le.transform(df_test_final[col])\n",
    "\n",
    "print(\"âœ… Label Encoding completed.\")\n",
    "\n",
    "\n",
    "# --- 3. One-Hot Encoding (Multi-Category Columns: 3+ Categories) ---\n",
    "\n",
    "# Remaining columns (those not label encoded) need One-Hot Encoding\n",
    "ohe_cols = [col for col in object_cols_train if col not in le_cols]\n",
    "print(f\"Applying One-Hot Encoding to {len(ohe_cols)} features.\")\n",
    "\n",
    "# Concatenate for safe One-Hot Encoding to ensure same number of columns in both sets\n",
    "# Drop TARGET before concatenation\n",
    "df_target = df_train_final['TARGET'] # Save the TARGET column\n",
    "df_combined = pd.concat([df_train_final.drop('TARGET', axis=1), df_test_final], axis=0)\n",
    "\n",
    "# Apply get_dummies to the combined dataset\n",
    "df_combined = pd.get_dummies(df_combined, columns=ohe_cols, dummy_na=False)\n",
    "\n",
    "# Re-separate the train and test sets\n",
    "df_train_final = df_combined.iloc[:len(df_train_final), :].copy()\n",
    "df_test_final = df_combined.iloc[len(df_train_final):, :].copy()\n",
    "\n",
    "# Add the TARGET column back to the training set\n",
    "df_train_final['TARGET'] = df_target\n",
    "\n",
    "# Clean up any potential duplicate columns (often created after merge/concat operations)\n",
    "df_train_final = df_train_final.loc[:,~df_train_final.columns.duplicated()]\n",
    "df_test_final = df_test_final.loc[:,~df_test_final.columns.duplicated()]\n",
    "\n",
    "print(f\"âœ… One-Hot Encoding completed.\")\n",
    "\n",
    "\n",
    "# --- 4. Final Verification and Preparation for Modeling ---\n",
    "print(f\"\\nFinal Train set shape: {df_train_final.shape}\")\n",
    "print(f\"Final Test set shape: {df_test_final.shape}\")\n",
    "\n",
    "print(f\"Total number of features after encoding: {df_train_final.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
