{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0728a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras) (3.10.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scikeras) (1.6.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (3.15.1)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.18.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.5.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=3.2.0->scikeras) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.13.0\n"
     ]
    }
   ],
   "source": [
    "# scikeras not itstalled in the colab environment\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6acdb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CORE LIBRARIES AND UTILITIES ---\n",
    "# Standard data manipulation and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# --- 2. SCIKIT-LEARN: MODEL SELECTION & METRICS ---\n",
    "# Tools for splitting data, cross-validation, and performance evaluation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    KFold, \n",
    "    cross_val_score, \n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline # For creating robust modeling workflows\n",
    "from sklearn.impute import SimpleImputer # Tool to handle NaNs\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "# --- 3. SCIKIT-LEARN: BASE CLASSIFIERS ---\n",
    "# Linear Models, Instance-Based, and Probabilistic Models\n",
    "from sklearn.linear_model import LogisticRegression # Linear Model\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # Discriminant Model\n",
    "from sklearn.naive_bayes import GaussianNB # Probabilistic Model\n",
    "from sklearn.neighbors import KNeighborsClassifier # Instance-based Model\n",
    "from sklearn.tree import DecisionTreeClassifier # Tree-based Model\n",
    "\n",
    "# Note: SVC (Support Vector Machine) is often excluded from rapid exploration \n",
    "# due to its high computational cost on large datasets.\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neural_network import MLPClassifier # Simple Neural Network\n",
    "\n",
    "# --- 4. SCIKIT-LEARN: ENSEMBLE MODELS ---\n",
    "# Advanced classifiers for improved performance (often the top performers)\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, \n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier, \n",
    "    ExtraTreesClassifier\n",
    ")\n",
    "# modern boosting libraries that often outperform Scikit-learn ensembles:\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# --- 5. DEEP LEARNING (Keras/TensorFlow) ---\n",
    "# Note: Using SciKeras for Scikit-learn compatibility is the modern best practice.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout # Added Dropout for robustness\n",
    "from scikeras.wrappers import KerasClassifier # Modern replacement\n",
    "from tensorflow.keras.optimizers import SGD, Adam # Adam is usually preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "388bc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1bbd0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully added 'src' directory to Python path.\n"
     ]
    }
   ],
   "source": [
    "# Define project Path in Colab\n",
    "PROJECT_BASE_PATH = '/content/drive/MyDrive/Project_01' \n",
    "\n",
    "# ADD 'src' DIRECTORY TO PYTHON PATH\n",
    "SRC_PATH = os.path.join(PROJECT_BASE_PATH, 'src')\n",
    "\n",
    "# verify if SRC_PATH is already in sys.path\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "    print(\"✅ Successfully added 'src' directory to Python path.\")\n",
    "\n",
    "# IMPORT Paths CLASS FROM config MODULE\n",
    "from config import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98ce8742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Project configuration (Paths) initialized successfully.\n",
      "Raw Data Path check: /content/drive/MyDrive/Project_01/data/raw/application_train.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from config import Paths\n",
    "    \n",
    "    # 3. Inicialize a instância com um nome único (cfg)\n",
    "    cfg = Paths(PROJECT_BASE_PATH) # <-- Mudança aqui\n",
    "    cfg.create_dirs() \n",
    "    \n",
    "    print(\"\\n✅ Project configuration (Paths) initialized successfully.\")\n",
    "    print(f\"Raw Data Path check: {cfg.TRAIN_RAW_FILE}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"❌ Error: Could not import Paths from config module.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0186bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Encoded Train Data. Shape: (307511, 266)\n",
      "✅ Loaded Encoded Test Data. Shape: (48744, 265)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FINAL_TRAIN_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'train_final_encoded.parquet')\n",
    "FINAL_TEST_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'test_final_encoded.parquet')\n",
    "\n",
    "try:\n",
    "    # Read the Parquet files\n",
    "    df_train_final = pd.read_parquet(FINAL_TRAIN_FILE)\n",
    "    df_test_final = pd.read_parquet(FINAL_TEST_FILE)\n",
    "    \n",
    "    print(f\"✅ Loaded Encoded Train Data. Shape: {df_train_final.shape}\")\n",
    "    print(f\"✅ Loaded Encoded Test Data. Shape: {df_test_final.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ ERROR: Parquet files not found. Ensure Block 13 was executed successfully and saved the files.\")\n",
    "    # Exit or raise an error if critical data is missing\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d1e0112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Data Diagnostics on Features (X) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All features appear to be numerical (int/float).\n",
      "⚠️ WARNING: X contains 1900440 NaN values and 0 Inf values.\n",
      "   The Universal Preprocessor Pipeline should handle these.\n",
      "✅ X converted to NumPy array for training.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Running Data Diagnostics on Features (X) ---\")\n",
    "\n",
    "# 1. Separate Features (X) and Target (y)\n",
    "EXCLUDED_COLS = ['SK_ID_CURR', 'TARGET', 'TIME_INDEX'] \n",
    "features = [col for col in df_train_final.columns if col not in EXCLUDED_COLS]\n",
    "\n",
    "X = df_train_final[features]\n",
    "y = df_train_final['TARGET']\n",
    "\n",
    "# 2. Check for Non-Numerical Data Types\n",
    "non_numeric_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(\"❌ CRITICAL ERROR: Found non-numerical columns in the feature matrix X.\")\n",
    "    print(\"   Please fix the following columns in Block 13 (Encoding):\")\n",
    "    print(non_numeric_cols)\n",
    "    # Stop execution to prevent further errors\n",
    "    raise ValueError(\"Non-numerical features found. Check Block 13.\")\n",
    "else:\n",
    "    print(\"✅ All features appear to be numerical (int/float).\")\n",
    "\n",
    "# 3. Final Check for Invalid Values (NaN/Inf) *before* the Pipeline\n",
    "# (The Pipeline should handle this, but checking the raw state is useful)\n",
    "total_nans = X.isna().sum().sum()\n",
    "total_infs = np.isinf(X).sum().sum()\n",
    "\n",
    "if total_nans > 0 or total_infs > 0:\n",
    "    print(f\"⚠️ WARNING: X contains {total_nans} NaN values and {total_infs} Inf values.\")\n",
    "    print(\"   The Universal Preprocessor Pipeline should handle these.\")\n",
    "else:\n",
    "    print(\"✅ No NaNs or Infs found in raw features (before pipeline).\")\n",
    "\n",
    "# 4. Convert X to a pure NumPy array for Scikit-learn (Safe measure)\n",
    "# This forces Pandas to resolve any underlying type issues.\n",
    "X = X.values \n",
    "print(\"✅ X converted to NumPy array for training.\")\n",
    "\n",
    "# (Continue with the KFold setup and training loop...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88755eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Cross-Validation ---\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the Inf/NaN Cleanup Step\n",
    "replace_inf = FunctionTransformer(lambda X: np.nan_to_num(X, nan=np.nan, posinf=np.nan, neginf=np.nan), \n",
    "                                  validate=False)\n",
    "\n",
    "# Define the Universal Preprocessor Pipeline (Inf/NaN Handling)\n",
    "universal_preprocessor = Pipeline([\n",
    "    ('inf_handler', replace_inf),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "\n",
    "# 1. Configuration and Model List \n",
    "models = [\n",
    "    # A. Linear & Scaled Models (Need Preprocessor + Scaler)\n",
    "    ('Logistic Regression (Scaled)', Pipeline([\n",
    "        ('preprocessor', universal_preprocessor),\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('classifier', LogisticRegression(solver='liblinear', random_state=42, C=0.01, max_iter=200))\n",
    "    ])),\n",
    "    ('K-Nearest Neighbors (Scaled)', Pipeline([\n",
    "        ('preprocessor', universal_preprocessor),\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('classifier', KNeighborsClassifier(n_neighbors=5))\n",
    "    ])),\n",
    "    \n",
    "    # B. Tree Models (Need Preprocessor for NaN handling)\n",
    "    ('Decision Tree (Clean)', Pipeline([\n",
    "        ('preprocessor', universal_preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "    ])),\n",
    "    ('Random Forest (Clean)', Pipeline([\n",
    "        ('preprocessor', universal_preprocessor),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1))\n",
    "    ])),\n",
    "\n",
    "    # C. Boosting Models (Run through Preprocessor for consistency)\n",
    "    ('LightGBM (Clean)', Pipeline([\n",
    "        ('preprocessor', universal_preprocessor),\n",
    "        ('classifier', lgb.LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1, n_estimators=500))\n",
    "    ])),\n",
    "    ('XGBoost (Clean)', Pipeline([\n",
    "        ('preprocessor', universal_preprocessor),\n",
    "        ('classifier', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1, n_estimators=500))\n",
    "    ])),\n",
    "]\n",
    "\n",
    "# Set up Cross-Validation strategy\n",
    "NFOLDS = 3 \n",
    "kfold = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "cv_results = []\n",
    "model_names = []\n",
    "training_times = []\n",
    "\n",
    "\n",
    "# 2. Training Loop with Cross-Validation\n",
    "\n",
    "print(\"--- Starting Model Cross-Validation ---\")\n",
    "\n",
    "for name, model in models:\n",
    "    start_time = time()\n",
    "    \n",
    "    try:\n",
    "        # Perform Cross-Validation using ROC AUC\n",
    "        scores = cross_val_score(model, X, y, cv=kfold, scoring='roc_auc', n_jobs=-1)\n",
    "        \n",
    "        end_time = time()\n",
    "        \n",
    "        # Append results\n",
    "        cv_results.append(scores)\n",
    "        model_names.append(name)\n",
    "        training_times.append(end_time - start_time)\n",
    "        \n",
    "        # Print interim results\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        print(f\"Model: {name}\")\n",
    "        print(f\"Mean AUC: {scores.mean():.4f} (Std Dev: {scores.std():.4f})\")\n",
    "        print(f\"Training Time: {(end_time - start_time):.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error encountered for model {name}: {e}\")\n",
    "        cv_results.append([np.nan] * NFOLDS)\n",
    "        model_names.append(name)\n",
    "        training_times.append(np.nan)\n",
    "\n",
    "\n",
    "# 3. Final Summary\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Mean_AUC': [np.mean(res) for res in cv_results],\n",
    "    'Std_Dev_AUC': [np.std(res) for res in cv_results],\n",
    "    'Time_Seconds': training_times\n",
    "}).sort_values(by='Mean_AUC', ascending=False)\n",
    "\n",
    "print(\"\\n--- Summary of Model Performance (Ranked by Mean AUC) ---\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
