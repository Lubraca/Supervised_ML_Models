{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc22b2ac",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d24c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import ipeadatapy as ip\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf36397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle and Ipeadata libraries if they haven't been installed in this session\n",
    "# Uncomment the line below if running in a fresh environment\n",
    "#%pip install kaggle ipeadatapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec80a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## talk abou kaggle tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8105a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If your Drive is already mounted via Colab's interface. If it is not, uncomment the line below.\n",
    "#from google.colab import drive\n",
    "# We skip the drive.mount() command here to avoid the Value Error.\n",
    "\n",
    "# Define the base path to your project folder on Google Drive\n",
    "# ATTENTION: Verify that this path leads to the folder containing 'dados_macro_brasil.csv'!\n",
    "#DRIVE_BASE_PATH = '/content/drive/MyDrive/Project_01' # <-- Adjust if your path is different!\n",
    "#MACRO_DATA_PATH = os.path.join(DRIVE_BASE_PATH, 'brasil_macro_data.csv')\n",
    "\n",
    "#print(\"Setup complete. Drive path defined at:\", DRIVE_BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf7bab",
   "metadata": {},
   "source": [
    "# Block 1: Kaggle Data Download and Unzip (Home Credit Default Risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd69fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download kaggle data via API it is necessary to configure your Token in kaggle profile. \n",
    "# After configuring your credentials run the code below in the terminal\n",
    "\n",
    "#mkdir -p ~/.kaggle\n",
    "## CRITICAL: Replace the entire string below with your key content\n",
    "#echo '{\"username\":\"YOUR_USERNAME\",\"key\":\"YOUR_LONG_API_KEY\"}' > ~/.kaggle/kaggle.json\n",
    "#chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc3a870b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created/checked: /content/drive/MyDrive/Project_01/data/raw/kaggle\n",
      "Attempting to download competition dataset: home-credit-default-risk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading home-credit-default-risk.zip to /content/drive/MyDrive/Project_01/data/raw/kaggle\n",
      "100% 687M/688M [00:07<00:00, 79.1MB/s]\n",
      "100% 688M/688M [00:07<00:00, 90.7MB/s]\n",
      "✅ Kaggle download command executed.\n",
      "\n",
      "Unzipping data from: /content/drive/MyDrive/Project_01/data/raw/kaggle/home-credit-default-risk.zip\n",
      "✅ Data unzipped to: /content/drive/MyDrive/Project_01/data/raw/kaggle/home-credit-default-risk\n",
      "Zip file removed.\n"
     ]
    }
   ],
   "source": [
    "DRIVE_BASE_PATH = '/content/drive/MyDrive/Project_01'\n",
    "\n",
    "# CRITICAL CORRECTION: Use the competition slug and command\n",
    "KAGGLE_COMPETITION_SLUG = 'home-credit-default-risk' \n",
    "\n",
    "# Path Variables (Ensure DRIVE_BASE_PATH is correct, e.g., '/content/drive/MyDrive/Project_01')\n",
    "KAGGLE_DIR = os.path.join(DRIVE_BASE_PATH, 'data/raw/kaggle') \n",
    "DATA_DIR_RAW = os.path.join(KAGGLE_DIR, KAGGLE_COMPETITION_SLUG)\n",
    "ZIP_FILE = os.path.join(KAGGLE_DIR, KAGGLE_COMPETITION_SLUG + '.zip')\n",
    "\n",
    "# 1. Create necessary directories\n",
    "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR_RAW, exist_ok=True)\n",
    "print(f\"Directories created/checked: {KAGGLE_DIR}\")\n",
    "\n",
    "# 2. Download the dataset using the Kaggle API \n",
    "print(f\"Attempting to download competition dataset: {KAGGLE_COMPETITION_SLUG}\")\n",
    "try:\n",
    "    # Use 'competitions download -c' syntax\n",
    "    !kaggle competitions download -c {KAGGLE_COMPETITION_SLUG} -p {KAGGLE_DIR} --force\n",
    "    print(\"✅ Kaggle download command executed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Kaggle download command failed unexpectedly. Error: {e}\")\n",
    "    # Note: If the 403 error persists, the problem is your API key/rules acceptance, not the command syntax.\n",
    "\n",
    "\n",
    "# 3. Unzip the downloaded file\n",
    "print(f\"\\nUnzipping data from: {ZIP_FILE}\")\n",
    "try:\n",
    "    with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR_RAW)\n",
    "    print(f\"✅ Data unzipped to: {DATA_DIR_RAW}\")\n",
    "    \n",
    "    # Clean up the zip file\n",
    "    os.remove(ZIP_FILE)\n",
    "    print(\"Zip file removed.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Zip file not found at {ZIP_FILE}. This means the download failed. Check API key/rules acceptance.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR during unzipping: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee4e3a",
   "metadata": {},
   "source": [
    "# Block 2: Query and save Brazil Central Bank inflation and interest rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e94eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory check complete: /content/drive/MyDrive/Project_01\n",
      "✅ Placeholder CSV created and saved at: /content/drive/MyDrive/Project_01/brasil_macro_data.csv\n",
      "You can now safely run the loading block (Block 2).\n"
     ]
    }
   ],
   "source": [
    "# Generate and Save Placeholder CSV\n",
    "\n",
    "# 1. Define the directory path (ensure the folder exists in your Google Drive)\n",
    "\n",
    "MACRO_DATA_PATH = os.path.join(DRIVE_BASE_PATH, 'brasil_macro_data.csv')\n",
    "\n",
    "# 2. Create the directory\n",
    "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
    "print(f\"Directory check complete: {DRIVE_BASE_PATH}\")\n",
    "\n",
    "# 3. Create a simple, empty DataFrame with the expected columns/index for now\n",
    "placeholder_data = {\n",
    "    'date': pd.to_datetime(['2023-01', '2023-02']),\n",
    "    'SELIC': [13.75, 13.65], \n",
    "    'IPCA': [5.79, 5.60] \n",
    "}\n",
    "df_placeholder = pd.DataFrame(placeholder_data)\n",
    "df_placeholder.set_index('date', inplace=True)\n",
    "\n",
    "# 4. Save the placeholder DataFrame to the expected path\n",
    "df_placeholder.to_csv(MACRO_DATA_PATH)\n",
    "\n",
    "print(f\"✅ Placeholder CSV created and saved at: {MACRO_DATA_PATH}\")\n",
    "print(\"You can now safely run the loading block (Block 2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e09c95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant for the BCB API limitation\n",
    "BCB_WINDOW_YEARS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c435ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Brazil Central Bank API\n",
    "\n",
    "def query_bc(serie_code, START_DATE_STR, END_DATE_STR):\n",
    "    \"\"\"\n",
    "    Auxiliary function to query the BCB API directly via URL for a specific time chunk.\n",
    "    Dates must be in DD/MM/YYYY format as required by the BCB API.\n",
    "    \"\"\"\n",
    "    url = f'https://api.bcb.gov.br/dados/serie/bcdata.sgs.{serie_code}/dados?formato=json&dataInicial={START_DATE_STR}&dataFinal={END_DATE_STR}'\n",
    "    \n",
    "    try:\n",
    "        # BCB returns 'data' and 'valor' columns\n",
    "        df = pd.read_json(url)\n",
    "        \n",
    "        if df.empty:\n",
    "             print(f\"Warning: BCB returned empty data for series {serie_code} from {START_DATE_STR} to {END_DATE_STR}.\")\n",
    "             return pd.DataFrame()\n",
    "             \n",
    "        df['date'] = pd.to_datetime(df['data'], dayfirst=True)\n",
    "        df = df.rename(columns={'valor': str(serie_code)})\n",
    "        df.set_index('date', inplace=True)\n",
    "        df = df[[str(serie_code)]] \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying BCB series {serie_code} from {START_DATE_STR} to {END_DATE_STR}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec39c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Function that query the data from Brazil Central Bank API, with a 10 year windown condition for querying\n",
    "\n",
    "def get_series_with_pagination(serie_code, global_start_date_str, global_end_date_str):\n",
    "    \"\"\"\n",
    "    Breaks the total time period into 10-year chunks and calls the BCB API for each chunk.\n",
    "    This bypasses the API's 10-year limit. Input dates must be in DD/MM/YYYY format.\n",
    "    \"\"\"\n",
    "    # Ensure all dates are the FIRST day of their respective month for safer querying\n",
    "    global_start_date = datetime.strptime(global_start_date_str, '%d/%m/%Y').replace(day=1)\n",
    "    global_end_date = datetime.strptime(global_end_date_str, '%d/%m/%Y').replace(day=1)\n",
    "    \n",
    "    all_data = []\n",
    "    current_end_date = global_end_date\n",
    "    \n",
    "    print(f\"Starting pagination for series {serie_code}. Total range: {global_start_date_str} to {global_end_date_str}\")\n",
    "    \n",
    "    while current_end_date >= global_start_date:\n",
    "        # Calculate start date 10 years earlier using relativedelta\n",
    "        current_start_date = current_end_date - relativedelta(years=BCB_WINDOW_YEARS)\n",
    "        \n",
    "        # Ensure that the start date of the chunk is the 1st day of the month\n",
    "        current_start_date = current_start_date.replace(day=1) \n",
    "        \n",
    "        # Ensure the chunk doesn't start before the user's global start date\n",
    "        if current_start_date < global_start_date:\n",
    "            current_start_date = global_start_date\n",
    "            \n",
    "        # Format dates for the query (DD/MM/YYYY)\n",
    "        query_start_date_str = current_start_date.strftime('%d/%m/%Y')\n",
    "        query_end_date_str = current_end_date.strftime('%d/%m/%Y')\n",
    "        \n",
    "        print(f\"  -> Querying chunk: {query_start_date_str} to {query_end_date_str}\")\n",
    "        \n",
    "        df_chunk = query_bc(serie_code, query_start_date_str, query_end_date_str)\n",
    "        \n",
    "        if not df_chunk.empty:\n",
    "            all_data.append(df_chunk)\n",
    "        \n",
    "        # Update the end date for the next iteration: move back one day from the start date of the current chunk\n",
    "        current_end_date = current_start_date - relativedelta(days=1)\n",
    "        \n",
    "        # Stop the loop if we've processed the earliest required data block\n",
    "        if current_start_date == global_start_date:\n",
    "            break\n",
    "\n",
    "    if not all_data:\n",
    "        print(f\"The query returned no data for series {serie_code}.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Concatenate all DataFrames, sort by date, and remove any duplicates\n",
    "    df_final = pd.concat(all_data).sort_index()\n",
    "    df_final = df_final[~df_final.index.duplicated(keep='first')]\n",
    "    print(f\"Pagination successful. Total data points: {len(df_final)}\")\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8bac1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring SELIC data...\n",
      "Starting pagination for series 432. Total range: 01/01/2014 to 01/11/2025\n",
      "  -> Querying chunk: 01/11/2015 to 01/11/2025\n",
      "  -> Querying chunk: 01/01/2014 to 31/10/2015\n",
      "Pagination successful. Total data points: 4323\n",
      "✅ SELIC Data acquired and paginated successfully.\n",
      "SELIC rows after resampling: 143\n"
     ]
    }
   ],
   "source": [
    "# Main Macro Data Acquisition (with Pagination)\n",
    "\n",
    "# --- USER INPUT ---\n",
    "# Define the global start and end date for the macro data acquisition (DD/MM/YYYY format)\n",
    "GLOBAL_START_DATE_STR = '01/01/2014' \n",
    "GLOBAL_END_DATE_STR = '01/11/2025'   \n",
    "# ------------------\n",
    "\n",
    "# 1. Acquire SELIC Rate (BCB Series 432) using Pagination\n",
    "print(\"Acquiring SELIC data...\")\n",
    "try:\n",
    "    # Use the pagination function to bypass the 10-year limit\n",
    "    df_selic = get_series_with_pagination(\n",
    "        432, \n",
    "        GLOBAL_START_DATE_STR, \n",
    "        GLOBAL_END_DATE_STR\n",
    "    )\n",
    "    df_selic = df_selic.resample('ME').last()\n",
    "    df_selic.index = df_selic.index.to_period('M') \n",
    "    df_selic.rename(columns={'432': 'SELIC'}, inplace=True)\n",
    "    print(\"✅ SELIC Data acquired and paginated successfully.\")\n",
    "    print(f\"SELIC rows after resampling: {len(df_selic)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR processing SELIC data: {e}\")\n",
    "    df_selic = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3014a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acquiring IPCA data...\n",
      "Starting pagination for series 433. Total range: 01/01/2014 to 01/11/2025\n",
      "  -> Querying chunk: 01/11/2015 to 01/11/2025\n",
      "  -> Querying chunk: 01/01/2014 to 31/10/2015\n",
      "Pagination successful. Total data points: 142\n",
      "✅ IPCA Data acquired successfully.\n"
     ]
    }
   ],
   "source": [
    "# Acquire IPCA (Brazilian Inflation Index - Ipeadata)\n",
    "print(\"\\nAcquiring IPCA data...\")\n",
    "try:\n",
    "    # Convert the end date string for the IPCA year filter\n",
    "    global_end_date_dt = datetime.strptime(GLOBAL_END_DATE_STR, '%d/%m/%Y')\n",
    "    \n",
    "    # IPCA Code GMEC12_IPCA12 (12-month accumulated)\n",
    "    df_ipca = get_series_with_pagination(\n",
    "        433, \n",
    "        GLOBAL_START_DATE_STR, \n",
    "        GLOBAL_END_DATE_STR\n",
    "    )\n",
    "    df_ipca.index = df_ipca.index.to_period('M') \n",
    "    df_ipca.rename(columns={'433': 'IPCA'}, inplace=True)\n",
    "    print(\"✅ IPCA Data acquired successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error acquiring IPCA data from Ipeadata: {e}\")\n",
    "    df_ipca = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd9dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Macro Data Merged and Saved:\n",
      "         IPCA  SELIC\n",
      "date                \n",
      "2014-01  0.55  10.50\n",
      "2014-02  0.69  10.75\n",
      "2014-03  0.92  10.75\n",
      "2014-04  0.67  11.00\n",
      "2014-05  0.46  11.00\n",
      "Macro Data Shape: (142, 2)\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets, clean, and save\n",
    "if not df_selic.empty and not df_ipca.empty:\n",
    "    df_macro = df_ipca.join(df_selic, how='outer') \n",
    "    df_macro = df_macro.ffill()\n",
    "    \n",
    "    # Save the data to Google Drive\n",
    "    df_macro.to_csv(MACRO_DATA_PATH)\n",
    "    \n",
    "    print(\"\\n✅ Final Macro Data Merged and Saved:\")\n",
    "    print(df_macro.head())\n",
    "    print(f\"Macro Data Shape: {df_macro.shape}\")\n",
    "else:\n",
    "    print(\"\\n❌ Could not merge data due to empty SELIC or IPCA datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a268f",
   "metadata": {},
   "source": [
    "# Simulate time variable for Credit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73d81d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load Micro Data from: /content/drive/MyDrive/Project_01/data/raw/kaggle/home-credit-default-risk/application_train.csv\n",
      "✅ Micro Data loaded and Time Index simulated successfully.\n",
      "   SK_ID_CURR  TARGET TIME_INDEX\n",
      "0      100002       1    2013-12\n",
      "1      100003       0    2013-06\n",
      "2      100004       0    2015-02\n",
      "3      100006       0    2014-03\n",
      "4      100007       0    2015-02\n",
      "\n",
      "Micro Data Shape: (307511, 123)\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the main training data\n",
    "RAW_TRAIN_FILE = os.path.join(DATA_DIR_RAW, 'application_train.csv')\n",
    "\n",
    "print(f\"Attempting to load Micro Data from: {RAW_TRAIN_FILE}\")\n",
    "\n",
    "try:\n",
    "    df_train = pd.read_csv(RAW_TRAIN_FILE)\n",
    "    \n",
    "    # 1. Simulate the Time Variable (Crucial for Macro Merge)\n",
    "    # The Home Credit data is cross-sectional (no monthly date). \n",
    "    # We must assign a random monthly period to each client \n",
    "    # for the purpose of joining with the monthly macro data (df_macro).\n",
    "    \n",
    "    # Range of dates for simulation (e.g., last 5 years relative to the data creation)\n",
    "    # The actual date of the dataset is not critical, only the relative time index.\n",
    "    start_date = datetime(2013, 1, 1)\n",
    "    end_date = datetime(2018, 5, 1) # Kaggle data was originally published around this time\n",
    "    \n",
    "    date_range = pd.period_range(start=start_date, end=end_date, freq='M')\n",
    "    \n",
    "    # Assign a random date from the range to each client\n",
    "    df_train['TIME_INDEX'] = random.choices(date_range, k=len(df_train))\n",
    "    df_train['TIME_INDEX'] = df_train['TIME_INDEX'].astype('object')\n",
    "    \n",
    "    # 2. Convert to Period Index for Joining\n",
    "    df_train['TIME_INDEX'] = df_train['TIME_INDEX'].apply(lambda x: pd.Period(x, freq='M'))\n",
    "    \n",
    "    print(\"✅ Micro Data loaded and Time Index simulated successfully.\")\n",
    "    print(df_train[['SK_ID_CURR', 'TARGET', 'TIME_INDEX']].head())\n",
    "    print(\"\\nMicro Data Shape:\", df_train.shape)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: File not found at {RAW_TRAIN_FILE}. The Kaggle download or unzipping may have failed.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during micro data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b59fe",
   "metadata": {},
   "source": [
    "# Load Macro Data and Prepare Time Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6f21511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load Macro Data from: /content/drive/MyDrive/Project_01/brasil_macro_data.csv\n",
      "✅ Macro Data loaded and Time Index prepared successfully.\n",
      "            IPCA  SELIC\n",
      "TIME_INDEX             \n",
      "2014-01     0.55  10.50\n",
      "2014-02     0.69  10.75\n",
      "2014-03     0.92  10.75\n",
      "2014-04     0.67  11.00\n",
      "2014-05     0.46  11.00\n",
      "\n",
      "Macro Data Shape: (142, 2)\n"
     ]
    }
   ],
   "source": [
    "# The path to the macro data file was defined in Block 1 (e.g., .../Project_01/brasil_macro_data.csv)\n",
    "print(f\"Attempting to load Macro Data from: {MACRO_DATA_PATH}\")\n",
    "\n",
    "try:\n",
    "    df_macro = pd.read_csv(MACRO_DATA_PATH)\n",
    "    \n",
    "    # 1. Convert the Date Column to Datetime\n",
    "    # We assume the time column is named 'DATE' (or 'date', depending on the API output/saving convention)\n",
    "    # Adjust 'DATE' if your CSV uses a different column name (e.g., 'date')\n",
    "    if 'DATE' in df_macro.columns:\n",
    "        df_macro['DATE'] = pd.to_datetime(df_macro['DATE'])\n",
    "    elif 'date' in df_macro.columns:\n",
    "        df_macro['DATE'] = pd.to_datetime(df_macro['date'])\n",
    "        df_macro = df_macro.drop(columns=['date']) # Clean up the old column if it was renamed\n",
    "    else:\n",
    "        raise ValueError(\"Time column ('DATE' or 'date') not found in macro data.\")\n",
    "\n",
    "\n",
    "    # 2. Convert to Period Index (Monthly Frequency - 'M')\n",
    "    # This conversion is fundamental for merging with df_train['TIME_INDEX']\n",
    "    df_macro['TIME_INDEX'] = df_macro['DATE'].dt.to_period('M')\n",
    "    \n",
    "    # 3. Drop the original datetime column\n",
    "    df_macro = df_macro.drop(columns=['DATE'])\n",
    "    \n",
    "    # Set TIME_INDEX as the index for easier joining (optional, but good practice)\n",
    "    df_macro = df_macro.set_index('TIME_INDEX')\n",
    "\n",
    "    # Display the result\n",
    "    print(\"✅ Macro Data loaded and Time Index prepared successfully.\")\n",
    "    print(df_macro.head())\n",
    "    print(\"\\nMacro Data Shape:\", df_macro.shape)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Macro file not found at {MACRO_DATA_PATH}. Check if the file is in your Google Drive.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during macro data loading or processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2f593",
   "metadata": {},
   "source": [
    "## Merging Macro and Micro home credit loan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0beee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting merge of micro (df_train) and macro (df_macro) data...\n",
      "✅ Datasets merged successfully.\n",
      "\n",
      "Enriched Data Sample (with new macro features):\n",
      "   SK_ID_CURR  TARGET TIME_INDEX  IPCA  SELIC\n",
      "0      100002       1    2013-12   NaN    NaN\n",
      "1      100003       0    2013-06   NaN    NaN\n",
      "2      100004       0    2015-02  1.22  12.25\n",
      "3      100006       0    2014-03  0.92  10.75\n",
      "4      100007       0    2015-02  1.22  12.25\n",
      "\n",
      "Final Enriched Shape: (307511, 125)\n"
     ]
    }
   ],
   "source": [
    "# Merge Micro and Macro Data df_train\n",
    "\n",
    "print(\"Starting merge of micro (df_train) and macro (df_macro) data...\")\n",
    "\n",
    "# The merge key is the TIME_INDEX (Period('M'))\n",
    "# We use a LEFT JOIN to keep all client rows from df_train and pull in the macro data.\n",
    "try:\n",
    "    # Ensure df_macro is set up for joining by index.\n",
    "    # The previous steps should have set df_macro.index to TIME_INDEX (Period('M')).\n",
    "    df_train_enriched = df_train.merge(\n",
    "        df_macro, \n",
    "        left_on='TIME_INDEX', \n",
    "        right_index=True,  # Merge df_macro based on its index (TIME_INDEX)\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Check for successful join\n",
    "    if df_train_enriched.isnull().any().any():\n",
    "        # Check if only the new macro columns have NaNs (expected for early dates)\n",
    "        new_cols = df_macro.columns\n",
    "        if df_train_enriched[new_cols].isnull().all().all():\n",
    "            print(\"⚠️ Warning: New macro columns contain NaNs. This is expected if the IPCA data did not cover the full range of TIME_INDEX simulation (e.g., prior to 2000).\")\n",
    "            \n",
    "    print(\"✅ Datasets merged successfully.\")\n",
    "    print(\"\\nEnriched Data Sample (with new macro features):\")\n",
    "    # Displaying the target and the newly merged macro features\n",
    "    print(df_train_enriched[['SK_ID_CURR', 'TARGET', 'TIME_INDEX', 'IPCA', 'SELIC']].head())\n",
    "    print(f\"\\nFinal Enriched Shape: {df_train_enriched.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during the merge process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abcae37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load Test Data from: /content/drive/MyDrive/Project_01/data/raw/kaggle/home-credit-default-risk/application_test.csv\n",
      "✅ Test Data loaded and Time Index simulated successfully.\n",
      "✅ Test Data merged with Macro Data successfully.\n",
      "\n",
      "Enriched Test Data Sample:\n",
      "   SK_ID_CURR TIME_INDEX  IPCA  SELIC\n",
      "0      100001    2018-05  0.40  14.25\n",
      "1      100005    2013-03   NaN    NaN\n",
      "2      100013    2017-10  0.42  14.25\n",
      "3      100028    2017-05  0.31  14.25\n",
      "4      100038    2018-02  0.32  14.25\n",
      "\n",
      "Final Enriched Test Shape: (48744, 124)\n"
     ]
    }
   ],
   "source": [
    "# Load and Enrich the Test Data (application_test.csv)\n",
    "\n",
    "# Define the file path for the main test data\n",
    "RAW_TEST_FILE = os.path.join(DATA_DIR_RAW, 'application_test.csv')\n",
    "\n",
    "print(f\"Attempting to load Test Data from: {RAW_TEST_FILE}\")\n",
    "\n",
    "try:\n",
    "    df_test = pd.read_csv(RAW_TEST_FILE)\n",
    "    \n",
    "    # 1. Simulate the Time Variable (CRITICAL: Use the same date range as df_train)\n",
    "    \n",
    "    # Define the range of dates used in Block 5 (e.g., 2013-01-01 to 2018-05-01)\n",
    "    # Ensure this matches the simulation period used for df_train!\n",
    "    start_date_sim = datetime(2013, 1, 1)\n",
    "    end_date_sim = datetime(2018, 5, 1) \n",
    "    date_range = pd.period_range(start=start_date_sim, end=end_date_sim, freq='M')\n",
    "    \n",
    "    # Assign a random date from the range to each client\n",
    "    df_test['TIME_INDEX'] = random.choices(date_range, k=len(df_test))\n",
    "    df_test['TIME_INDEX'] = df_test['TIME_INDEX'].astype('object')\n",
    "    \n",
    "    # Convert to Period Index for Joining\n",
    "    df_test['TIME_INDEX'] = df_test['TIME_INDEX'].apply(lambda x: pd.Period(x, freq='M'))\n",
    "    \n",
    "    print(\"✅ Test Data loaded and Time Index simulated successfully.\")\n",
    "    \n",
    "    \n",
    "    # 2. Merge with the pre-processed Macro Data (df_macro)\n",
    "    \n",
    "    # df_macro was prepared in Block 4 and has TIME_INDEX as its actual index\n",
    "    df_test_enriched = df_test.merge(\n",
    "        df_macro, \n",
    "        left_on='TIME_INDEX', \n",
    "        right_index=True,  # Merge df_macro based on its index (TIME_INDEX)\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Test Data merged with Macro Data successfully.\")\n",
    "    print(\"\\nEnriched Test Data Sample:\")\n",
    "    print(df_test_enriched[['SK_ID_CURR', 'TIME_INDEX', 'IPCA', 'SELIC']].head())\n",
    "    print(f\"\\nFinal Enriched Test Shape: {df_test_enriched.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Test file not found at {RAW_TEST_FILE}. Check the Kaggle download path.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during test data loading or merging: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35f113",
   "metadata": {},
   "source": [
    "## Treat Missing values and saving processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "395c7cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NaN treatment with Group-wise Mean Imputation (Macro) and Median/Mode (Micro)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing categorical features with 'Missing'...\n",
      "✅ Categorical features imputed.\n",
      "Imputing macro features with Group-wise Mean (by Month)...\n",
      "✅ Macro features imputed with Group-wise Mean.\n",
      "Imputing numerical micro features with Median...\n",
      "\n",
      "Final Check - Train Remaining NaNs: 0\n",
      "Final Check - Test Remaining NaNs: 0\n",
      "\n",
      "Saving treated and enriched datasets...\n",
      "✅ Saved treated training data to: /content/drive/MyDrive/Project_01/data/processed/train_enriched.csv\n",
      "✅ Saved treated testing data to: /content/drive/MyDrive/Project_01/data/processed/test_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "# NaN Treatment: Group-wise Mean Imputation for Macro Features and Median/Mode for Micro Features\n",
    "print(\"Starting NaN treatment with Group-wise Mean Imputation (Macro) and Median/Mode (Micro)...\")\n",
    "\n",
    "# Identify Columns\n",
    "EXCLUDE_COLS = ['SK_ID_CURR', 'TARGET', 'TIME_INDEX'] \n",
    "macro_cols = ['SELIC', 'IPCA']\n",
    "all_cols = df_train_enriched.columns.tolist() \n",
    "\n",
    "# Columns that are purely micro features (excluding IDs, TARGET, and Macro columns)\n",
    "original_micro_cols = [col for col in all_cols if col not in EXCLUDE_COLS + macro_cols]\n",
    "\n",
    "# Identify column types among the micro features\n",
    "object_micro_cols = df_train_enriched[original_micro_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_micro_cols = [col for col in original_micro_cols if col not in object_micro_cols]\n",
    "\n",
    "\n",
    "# Categorical Imputation (Micro Features)\n",
    "\n",
    "print(\"Imputing categorical features with 'Missing'...\")\n",
    "\n",
    "# Impute Categorical in Train\n",
    "for col in object_micro_cols:\n",
    "    df_train_enriched[col] = df_train_enriched[col].fillna('Missing') \n",
    "\n",
    "# Impute Categorical in Test\n",
    "for col in object_micro_cols:\n",
    "    df_test_enriched[col] = df_test_enriched[col].fillna('Missing') \n",
    "    \n",
    "print(\"✅ Categorical features imputed.\")\n",
    "\n",
    "\n",
    "# Group-Wise Mean Imputation (Macro Features)\n",
    "\n",
    "print(\"Imputing macro features with Group-wise Mean (by Month)...\")\n",
    "\n",
    "# Create a temporary 'Month' column (required for Period Index grouping)\n",
    "df_train_enriched['Month'] = df_train_enriched['TIME_INDEX'].apply(lambda x: x.month)\n",
    "df_test_enriched['Month'] = df_test_enriched['TIME_INDEX'].apply(lambda x: x.month)\n",
    "\n",
    "# Calculate the monthly average ONLY from the TRAINING set (to prevent data leakage)\n",
    "monthly_mean_map = {}\n",
    "for col in macro_cols:\n",
    "    # Calculate the mean of the column grouped by 'Month'\n",
    "    monthly_mean_map[col] = df_train_enriched.groupby('Month')[col].mean()\n",
    "\n",
    "# Impute Train\n",
    "for col in macro_cols:\n",
    "    # If the value is NaN, replace it with the mean of the corresponding Month\n",
    "    df_train_enriched[col] = df_train_enriched.apply(\n",
    "        lambda row: monthly_mean_map[col].loc[row['Month']] if pd.isna(row[col]) else row[col],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Impute Test (Using the mean map calculated from TRAIN)\n",
    "for col in macro_cols:\n",
    "    # If the value is NaN, replace it with the mean of the corresponding Month from the TRAIN map\n",
    "    df_test_enriched[col] = df_test_enriched.apply(\n",
    "        lambda row: monthly_mean_map[col].loc[row['Month']] if pd.isna(row[col]) else row[col],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Drop the temporary 'Month' column\n",
    "df_train_enriched.drop('Month', axis=1, inplace=True)\n",
    "df_test_enriched.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "print(\"✅ Macro features imputed with Group-wise Mean.\")\n",
    "\n",
    "\n",
    "# Median Imputation (Original Numerical Micro Features)\n",
    "\n",
    "print(\"Imputing numerical micro features with Median...\")\n",
    "\n",
    "# Impute Train\n",
    "# Calculate the median ONLY on the training set\n",
    "train_medians = df_train_enriched[numeric_micro_cols].median()\n",
    "df_train_enriched[numeric_micro_cols] = df_train_enriched[numeric_micro_cols].fillna(train_medians)\n",
    "\n",
    "# Impute Test (Using the medians calculated from TRAIN)\n",
    "df_test_enriched[numeric_micro_cols] = df_test_enriched[numeric_micro_cols].fillna(train_medians) \n",
    "\n",
    "\n",
    "# Final Verification and Save\n",
    "\n",
    "print(f\"\\nFinal Check - Train Remaining NaNs: {df_train_enriched.isnull().sum().sum()}\")\n",
    "print(f\"Final Check - Test Remaining NaNs: {df_test_enriched.isnull().sum().sum()}\")\n",
    "\n",
    "\n",
    "DATA_DIR_PROCESSED = os.path.join(DRIVE_BASE_PATH, 'data', 'processed')\n",
    "if not os.path.exists(DATA_DIR_PROCESSED):\n",
    "    os.makedirs(DATA_DIR_PROCESSED)\n",
    "\n",
    "TRAIN_PROCESSED_FILE = os.path.join(DATA_DIR_PROCESSED, 'train_enriched.csv')\n",
    "TEST_PROCESSED_FILE = os.path.join(DATA_DIR_PROCESSED, 'test_enriched.csv')\n",
    "\n",
    "print(\"\\nSaving treated and enriched datasets...\")\n",
    "\n",
    "try:\n",
    "    df_train_enriched.to_csv(TRAIN_PROCESSED_FILE, index=False)\n",
    "    print(f\"✅ Saved treated training data to: {TRAIN_PROCESSED_FILE}\")\n",
    "\n",
    "    df_test_enriched.to_csv(TEST_PROCESSED_FILE, index=False)\n",
    "    print(f\"✅ Saved treated testing data to: {TEST_PROCESSED_FILE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR saving files: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
