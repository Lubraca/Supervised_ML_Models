{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafdeb7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2692ba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.2)\n",
      "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install category_encoders if not already installed in the environment\n",
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d0b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully added 'src' directory to Python path.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define project Path in Colab\n",
    "PROJECT_BASE_PATH = '/content/drive/MyDrive/Project_01' \n",
    "\n",
    "# ADD 'src' DIRECTORY TO PYTHON PATH\n",
    "SRC_PATH = os.path.join(PROJECT_BASE_PATH, 'src')\n",
    "\n",
    "# verify if SRC_PATH is already in sys.path\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "    print(\"âœ… Successfully added 'src' directory to Python path.\")\n",
    "\n",
    "# IMPORT Paths CLASS FROM config MODULE\n",
    "from config import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8182dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Project configuration (Paths) initialized successfully.\n",
      "Raw Data Path check: /content/drive/MyDrive/Project_01/data/raw/application_train.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from config import Paths\n",
    "    \n",
    "    # 3. Inicialize a instÃ¢ncia com um nome Ãºnico (cfg)\n",
    "    cfg = Paths(PROJECT_BASE_PATH) # <-- MudanÃ§a aqui\n",
    "    cfg.create_dirs() \n",
    "    \n",
    "    print(\"\\nâœ… Project configuration (Paths) initialized successfully.\")\n",
    "    print(f\"Raw Data Path check: {cfg.TRAIN_RAW_FILE}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âŒ Error: Could not import Paths from config module.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92cf6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PROCESSED = os.path.join(PROJECT_BASE_PATH, 'data', 'processed')\n",
    "TRAIN_PROCESSED_FILE = os.path.join(DATA_DIR_PROCESSED, 'train_enriched.csv')\n",
    "TEST_PROCESSED_FILE = os.path.join(DATA_DIR_PROCESSED, 'test_enriched.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68db1350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded final training data. Shape: (307511, 125)\n",
      "âœ… Loaded final testing data. Shape: (48744, 124)\n",
      "--- Starting Feature Engineering ---\n",
      "âœ… TRAIN: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\n",
      "âœ… TRAIN: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\n",
      "\n",
      "Training set shape after Macro Feature Engineering: (307511, 133)\n",
      "âœ… TEST: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\n",
      "âœ… TEST: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\n",
      "\n",
      "Testing set shape after Macro Feature Engineering: (48744, 132)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # --- LOAD TRAIN DATA USING CONFIG PATHS ---\n",
    "    df_train_final = pd.read_csv(cfg.TRAIN_PROCESSED_FILE)\n",
    "    \n",
    "    # ESSENTIAL FIX: Ensures TIME_INDEX is of temporal type after loading from CSV\n",
    "    df_train_final['TIME_INDEX'] = pd.to_datetime(df_train_final['TIME_INDEX'])\n",
    "    \n",
    "    print(f\"âœ… Loaded final training data. Shape: {df_train_final.shape}\")\n",
    "\n",
    "    # --- LOAD TEST DATA USING CONFIG PATHS ---\n",
    "    df_test_final = pd.read_csv(cfg.TEST_PROCESSED_FILE)\n",
    "    \n",
    "    # Apply the same temporal conversion to the Test Set\n",
    "    df_test_final['TIME_INDEX'] = pd.to_datetime(df_test_final['TIME_INDEX'])\n",
    "    \n",
    "    print(f\"âœ… Loaded final testing data. Shape: {df_test_final.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: Final processed file not found. Check if Block 9 was run and files exist at: {cfg.DATA_PROCESSED_DIR}\")\n",
    "    df_train_final = None\n",
    "    df_test_final = None\n",
    "if df_train_final is not None and df_test_final is not None:\n",
    "    print(\"--- Starting Feature Engineering ---\")\n",
    "    \n",
    "    # --- 1. Create Lagged and Dynamic Macro Features (TRAIN SET) ---\n",
    "    \n",
    "    # KeyError/SetIndex Fix: Ensures TIME_INDEX is a regular column before setting\n",
    "    if 'TIME_INDEX' not in df_train_final.columns and 'TIME_INDEX' in df_train_final.index.names:\n",
    "        df_train_final = df_train_final.reset_index(level='TIME_INDEX')\n",
    "        print(\"ðŸ’¡ TRAIN: TIME_INDEX restored from index to column.\")\n",
    "    \n",
    "    # Temporarily set TIME_INDEX as the index for time-series operations\n",
    "    df_train_final = df_train_final.set_index('TIME_INDEX')\n",
    "    \n",
    "    # Use the correct column names\n",
    "    macro_features_to_engineer = ['SELIC', 'IPCA'] \n",
    "    \n",
    "    for col in macro_features_to_engineer:\n",
    "        # A) Lag (Previous Month's Value)\n",
    "        df_train_final[f'{col}_LAG1'] = df_train_final.groupby('SK_ID_CURR')[col].shift(1)\n",
    "\n",
    "        # B) Change (Current Month - Previous Month)\n",
    "        df_train_final[f'{col}_CHANGE'] = df_train_final[col] - df_train_final[f'{col}_LAG1']\n",
    "        \n",
    "        # C) Trend (3-Month Rolling Mean)\n",
    "        df_train_final[f'{col}_ROLLING_MEAN3'] = df_train_final.groupby('SK_ID_CURR')[col].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        \n",
    "    print(f\"âœ… TRAIN: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\")\n",
    "\n",
    "    # --- 2. Temporal Features from TIME_INDEX (TRAIN SET) ---\n",
    "    \n",
    "    # Extract month and year using the already defined index\n",
    "    df_train_final['MONTH_OF_YEAR'] = df_train_final.index.to_series().dt.month\n",
    "    df_train_final['YEAR'] = df_train_final.index.to_series().dt.year\n",
    "    \n",
    "    print(\"âœ… TRAIN: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\")\n",
    "\n",
    "    # Restore TIME_INDEX as a regular column\n",
    "    df_train_final = df_train_final.reset_index()\n",
    "\n",
    "    print(f\"\\nTraining set shape after Macro Feature Engineering: {df_train_final.shape}\")\n",
    "    \n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --- REPEAT FOR TEST SET ---\n",
    "    # --------------------------------------------------------------------------------\n",
    "    \n",
    "    # KeyError/SetIndex Fix for the Test Set\n",
    "    if 'TIME_INDEX' not in df_test_final.columns and 'TIME_INDEX' in df_test_final.index.names:\n",
    "        df_test_final = df_test_final.reset_index(level='TIME_INDEX')\n",
    "        print(\"ðŸ’¡ TEST: TIME_INDEX restored from index to column.\")\n",
    "    \n",
    "    # Temporarily set TIME_INDEX as the index for time-series operations\n",
    "    df_test_final = df_test_final.set_index('TIME_INDEX')\n",
    "    \n",
    "    for col in macro_features_to_engineer:\n",
    "        df_test_final[f'{col}_LAG1'] = df_test_final.groupby('SK_ID_CURR')[col].shift(1)\n",
    "        df_test_final[f'{col}_CHANGE'] = df_test_final[col] - df_test_final[f'{col}_LAG1']\n",
    "        df_test_final[f'{col}_ROLLING_MEAN3'] = df_test_final.groupby('SK_ID_CURR')[col].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        \n",
    "    print(f\"âœ… TEST: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\")\n",
    "\n",
    "    # Temporal Features (TEST SET)\n",
    "    df_test_final['MONTH_OF_YEAR'] = df_test_final.index.to_series().dt.month\n",
    "    df_test_final['YEAR'] = df_test_final.index.to_series().dt.year\n",
    "    \n",
    "    print(\"âœ… TEST: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\")\n",
    "\n",
    "    # Restore TIME_INDEX as a regular column\n",
    "    df_test_final = df_test_final.reset_index()\n",
    "\n",
    "    print(f\"\\nTesting set shape after Macro Feature Engineering: {df_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4f5c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Micro Feature Engineering ---\n",
      "âœ… DAYS_EMPLOYED anomaly fixed and replaced with NaN.\n",
      "âœ… Created 3 core ratio features.\n",
      "âœ… TIME_INDEX converted to numerical YEAR and original index removed.\n",
      "\n",
      "Training set shape after revised feature engineering: (307511, 135)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Micro Feature Engineering ---\")\n",
    "\n",
    "# Treat DAYS_EMPLOYED anomaly (365243 days â‰ˆ 1000 years) as missing (NaN) and flag it\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# --- IMPORTANT DATA CLEANING NOTE: DAYS_EMPLOYED ANOMALY ---\n",
    "# The value 365243 in DAYS_EMPLOYED is a known data anomaly specific to the Home Credit dataset.\n",
    "# It represents approximately 1000 years, and is used by the client/bank to code for\n",
    "# applicants who are currently **unemployed** or whose employment status is unverified.\n",
    "#\n",
    "# Best Practice Treatment:\n",
    "# 1. Replace the anomalous value (365243) with **NaN** to treat it as a missing value.\n",
    "# 2. Create a new **binary feature** (DAYS_EMPLOYED_ANOM) to allow the model (like LightGBM)\n",
    "#    to explicitly learn the predictive power of this specific 'unemployed/anomaly' group.\n",
    "# This prevents the model from interpreting 365243 as a literal, extremely long employment history.\n",
    "# -----------------------------------------------------------\n",
    "# --- 1. Anomaly Treatment and Transformation ---\n",
    "\n",
    "# 1. Fix the DAYS_EMPLOYED Anomaly\n",
    "# Replace the extreme positive value with NaN for proper imputation later.\n",
    "DAYS_EMPLOYED_ANOMALY = 365243 \n",
    "df_train_final['DAYS_EMPLOYED'].replace({DAYS_EMPLOYED_ANOMALY: np.nan})\n",
    "df_test_final['DAYS_EMPLOYED'].replace({DAYS_EMPLOYED_ANOMALY: np.nan})\n",
    "print(\"âœ… DAYS_EMPLOYED anomaly fixed and replaced with NaN.\")\n",
    "\n",
    "# 2. Create Simple Ratio Features (Crucial for risk assessment)\n",
    "# These are highly predictive and do not increase dimensionality.\n",
    "df_train_final['CREDIT_INCOME_RATIO'] = df_train_final['AMT_CREDIT'] / df_train_final['AMT_INCOME_TOTAL']\n",
    "df_test_final['CREDIT_INCOME_RATIO'] = df_test_final['AMT_CREDIT'] / df_test_final['AMT_INCOME_TOTAL']\n",
    "\n",
    "df_train_final['ANNUITY_INCOME_RATIO'] = df_train_final['AMT_ANNUITY'] / df_train_final['AMT_INCOME_TOTAL']\n",
    "df_test_final['ANNUITY_INCOME_RATIO'] = df_test_final['AMT_ANNUITY'] / df_test_final['AMT_INCOME_TOTAL']\n",
    "\n",
    "df_train_final['PAYMENT_RATE'] = df_train_final['AMT_ANNUITY'] / df_train_final['AMT_CREDIT']\n",
    "df_test_final['PAYMENT_RATE'] = df_test_final['AMT_ANNUITY'] / df_test_final['AMT_CREDIT']\n",
    "\n",
    "print(\"âœ… Created 3 core ratio features.\")\n",
    "\n",
    "# Handle TIME_INDEX\n",
    "\n",
    "if 'TIME_INDEX' in df_train_final.columns:\n",
    "    # Convert 'YYYY-MM' string to YYYY * 12 + MM, or simply the Year (simple numerical feature)\n",
    "    try:\n",
    "        df_train_final['YEAR'] = pd.to_datetime(df_train_final['TIME_INDEX']).dt.year\n",
    "        df_test_final['YEAR'] = pd.to_datetime(df_test_final['TIME_INDEX']).dt.year\n",
    "        \n",
    "        # Drop the original string TIME_INDEX to prevent errors in Block 13\n",
    "        df_train_final = df_train_final.drop(columns=['TIME_INDEX'])\n",
    "        df_test_final = df_test_final.drop(columns=['TIME_INDEX'])\n",
    "        \n",
    "        print(\"âœ… TIME_INDEX converted to numerical YEAR and original index removed.\")\n",
    "    except Exception:\n",
    "        print(\"âŒ WARNING: Could not convert TIME_INDEX to datetime. Dropping TIME_INDEX.\")\n",
    "        df_train_final = df_train_final.drop(columns=['TIME_INDEX'])\n",
    "        df_test_final = df_test_final.drop(columns=['TIME_INDEX'])\n",
    "\n",
    "\n",
    "print(f\"\\nTraining set shape after revised feature engineering: {df_train_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807ae5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed72a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting: K-Fold Target Encoding ---\n",
      "Found 16 categorical features to encode.\n",
      "  > Encoded NAME_CONTRACT_TYPE with K-Fold CV.\n",
      "  > Encoded CODE_GENDER with K-Fold CV.\n",
      "  > Encoded FLAG_OWN_CAR with K-Fold CV.\n",
      "  > Encoded FLAG_OWN_REALTY with K-Fold CV.\n",
      "  > Encoded NAME_TYPE_SUITE with K-Fold CV.\n",
      "  > Encoded NAME_INCOME_TYPE with K-Fold CV.\n",
      "  > Encoded NAME_EDUCATION_TYPE with K-Fold CV.\n",
      "  > Encoded NAME_FAMILY_STATUS with K-Fold CV.\n",
      "  > Encoded NAME_HOUSING_TYPE with K-Fold CV.\n",
      "  > Encoded OCCUPATION_TYPE with K-Fold CV.\n",
      "  > Encoded WEEKDAY_APPR_PROCESS_START with K-Fold CV.\n",
      "  > Encoded ORGANIZATION_TYPE with K-Fold CV.\n",
      "  > Encoded FONDKAPREMONT_MODE with K-Fold CV.\n",
      "  > Encoded HOUSETYPE_MODE with K-Fold CV.\n",
      "  > Encoded WALLSMATERIAL_MODE with K-Fold CV.\n",
      "  > Encoded EMERGENCYSTATE_MODE with K-Fold CV.\n",
      "\n",
      "âœ… Target Encoding Complete. Final Feature Count:\n",
      "  > Train Shape: (307511, 135)\n",
      "  > Test Shape: (48744, 134)\n"
     ]
    }
   ],
   "source": [
    "# Target Encoding and Final Cleanup\n",
    "\n",
    "# Imports are omitted as requested, assuming TargetEncoder, KFold, pd, np, and re are imported.\n",
    "\n",
    "print(\"--- Starting: K-Fold Target Encoding ---\")\n",
    "\n",
    "# 1. Identify Categorical Features\n",
    "# Select all object (string) and category columns\n",
    "categorical_features = df_train_final.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Store the target column and the target values\n",
    "TARGET_COLUMN = 'TARGET'\n",
    "y_train = df_train_final[TARGET_COLUMN]\n",
    "print(f\"Found {len(categorical_features)} categorical features to encode.\")\n",
    "\n",
    "\n",
    "# 2. Target Encoding Setup (K-Fold CV)\n",
    "NFOLDS = 5\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty DataFrame to store the new encoded features (using copies for safety)\n",
    "df_train_encoded = df_train_final.copy() \n",
    "\n",
    "\n",
    "# --- 3. Apply K-Fold Target Encoding to TRAIN SET ---\n",
    "for col in categorical_features:\n",
    "    # Create a new feature column for the encoded mean\n",
    "    df_train_encoded[f'{col}_TARGET_ENC'] = np.nan\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(df_train_final, y_train)):\n",
    "        # Encoder is fitted on the remaining K-1 folds (train_index)\n",
    "        encoder = TargetEncoder(cols=[col])\n",
    "        encoder.fit(df_train_final.iloc[train_index], y_train.iloc[train_index])\n",
    "        \n",
    "        # Encoder is applied to the current fold (val_index)\n",
    "        df_train_encoded.loc[val_index, f'{col}_TARGET_ENC'] = encoder.transform(df_train_final.iloc[val_index])[col]\n",
    "        \n",
    "    print(f\"  > Encoded {col} with K-Fold CV.\")\n",
    "\n",
    "# --- 4. Apply Encoding to TEST SET (FIXED: Isolate Categoricals) ---\n",
    "\n",
    "# a. Re-fit the encoder on the *entire* original training set's categorical columns\n",
    "final_encoder = TargetEncoder(cols=categorical_features)\n",
    "final_encoder.fit(df_train_final[categorical_features], y_train) \n",
    "\n",
    "# b. Transform ONLY the categorical columns of the test set\n",
    "df_test_categoricals = df_test_final[categorical_features]\n",
    "df_test_transformed = final_encoder.transform(df_test_categoricals)\n",
    "\n",
    "# c. Rename the new columns to match the train set names\n",
    "rename_map = {col: f'{col}_TARGET_ENC' for col in categorical_features}\n",
    "df_test_transformed.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# d. Drop original categorical columns from the test frame and merge the new features\n",
    "df_test_final = df_test_final.drop(columns=categorical_features)\n",
    "df_test_final = pd.merge(\n",
    "    df_test_final, \n",
    "    df_test_transformed, \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Drop Original Categorical Columns from TRAIN SET\n",
    "df_train_final = df_train_encoded.drop(columns=categorical_features)\n",
    "\n",
    "\n",
    "# 6. Final Cleanup: LightGBM Name Safety\n",
    "# Function to remove problematic characters that break LightGBM/XGBoost (like [,] <, etc.)\n",
    "def clean_names(df):\n",
    "    cols = df.columns\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        new_col = re.sub(r'[^A-Za-z0-9_]+', '', col)\n",
    "        new_cols.append(new_col)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "df_train_final = clean_names(df_train_final)\n",
    "df_test_final = clean_names(df_test_final)\n",
    "\n",
    "print(f\"\\nâœ… Target Encoding Complete. Final Feature Count:\")\n",
    "print(f\"  > Train Shape: {df_train_final.shape}\")\n",
    "print(f\"  > Test Shape: {df_test_final.shape}\")\n",
    "\n",
    "# Optional: You can now save the final DataFrames as Parquet files here \n",
    "# before proceeding to Block 15 (Model Training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa10bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Final Processed DataFrames (Parquet Format) ---\n",
      "âœ… Final Train set saved to: /content/drive/MyDrive/Project_01/data/processed/train_final_encoded.parquet\n",
      "âœ… Final Test set saved to: /content/drive/MyDrive/Project_01/data/processed/test_final_encoded.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Saving Final Processed DataFrames (Parquet Format) ---\")\n",
    "\n",
    "# Define the save paths using the 'cfg' instance (assuming cfg.DATA_PROCESSED_DIR exists)\n",
    "FINAL_TRAIN_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'train_final_encoded.parquet')\n",
    "FINAL_TEST_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'test_final_encoded.parquet')\n",
    "\n",
    "\n",
    "# 1. Save the Training DataFrame\n",
    "# Use index=False as 'TIME_INDEX' should now be a regular column after Block 13\n",
    "df_train_final.to_parquet(FINAL_TRAIN_FILE, index=False)\n",
    "print(f\"âœ… Final Train set saved to: {FINAL_TRAIN_FILE}\")\n",
    "\n",
    "# 2. Save the Test DataFrame\n",
    "df_test_final.to_parquet(FINAL_TEST_FILE, index=False)\n",
    "print(f\"âœ… Final Test set saved to: {FINAL_TEST_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
