{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc22b2ac",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17d24c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import ipeadatapy as ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddf36397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
      "Requirement already satisfied: ipeadatapy in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ipeadatapy) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->ipeadatapy) (2.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ipeadatapy) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ipeadatapy) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "# Install Kaggle and Ipeadata libraries if they haven't been installed in this session\n",
    "# Uncomment the line below if running in a fresh environment\n",
    "%pip install kaggle ipeadatapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec80a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## talk abou kaggle tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8105a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If your Drive is already mounted via Colab's interface. If it is not, uncomment the line below.\n",
    "#from google.colab import drive\n",
    "# We skip the drive.mount() command here to avoid the Value Error.\n",
    "\n",
    "# Define the base path to your project folder on Google Drive\n",
    "# ATTENTION: Verify that this path leads to the folder containing 'dados_macro_brasil.csv'!\n",
    "#DRIVE_BASE_PATH = '/content/drive/MyDrive/Project_01' # <-- Adjust if your path is different!\n",
    "#MACRO_DATA_PATH = os.path.join(DRIVE_BASE_PATH, 'brasil_macro_data.csv')\n",
    "\n",
    "#print(\"Setup complete. Drive path defined at:\", DRIVE_BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf7bab",
   "metadata": {},
   "source": [
    "# Block 1: Kaggle Data Download and Unzip (Home Credit Default Risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc3a870b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created/checked: /content/drive/MyDrive/Project_01/data/raw/kaggle\n",
      "Attempting to download competition dataset: home-credit-default-risk\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
      "    from kaggle.cli import main\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/__init__.py\", line 6, in <module>\n",
      "    api.authenticate()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n",
      "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
      "OSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n",
      "✅ Kaggle download command executed.\n",
      "\n",
      "Unzipping data from: /content/drive/MyDrive/Project_01/data/raw/kaggle/home-credit-default-risk.zip\n",
      "❌ ERROR: Zip file not found at /content/drive/MyDrive/Project_01/data/raw/kaggle/home-credit-default-risk.zip. This means the download failed. Check API key/rules acceptance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# CRITICAL CORRECTION: Use the competition slug and command\n",
    "KAGGLE_COMPETITION_SLUG = 'home-credit-default-risk' \n",
    "\n",
    "# Path Variables (Ensure DRIVE_BASE_PATH is correct, e.g., '/content/drive/MyDrive/Project_01')\n",
    "KAGGLE_DIR = os.path.join(DRIVE_BASE_PATH, 'data/raw/kaggle') \n",
    "DATA_DIR_RAW = os.path.join(KAGGLE_DIR, KAGGLE_COMPETITION_SLUG)\n",
    "ZIP_FILE = os.path.join(KAGGLE_DIR, KAGGLE_COMPETITION_SLUG + '.zip')\n",
    "\n",
    "# 1. Create necessary directories\n",
    "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR_RAW, exist_ok=True)\n",
    "print(f\"Directories created/checked: {KAGGLE_DIR}\")\n",
    "\n",
    "# 2. Download the dataset using the Kaggle API \n",
    "print(f\"Attempting to download competition dataset: {KAGGLE_COMPETITION_SLUG}\")\n",
    "try:\n",
    "    # Use 'competitions download -c' syntax\n",
    "    !kaggle competitions download -c {KAGGLE_COMPETITION_SLUG} -p {KAGGLE_DIR} --force\n",
    "    print(\"✅ Kaggle download command executed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Kaggle download command failed unexpectedly. Error: {e}\")\n",
    "    # Note: If the 403 error persists, the problem is your API key/rules acceptance, not the command syntax.\n",
    "\n",
    "\n",
    "# 3. Unzip the downloaded file\n",
    "print(f\"\\nUnzipping data from: {ZIP_FILE}\")\n",
    "try:\n",
    "    with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR_RAW)\n",
    "    print(f\"✅ Data unzipped to: {DATA_DIR_RAW}\")\n",
    "    \n",
    "    # Clean up the zip file\n",
    "    os.remove(ZIP_FILE)\n",
    "    print(\"Zip file removed.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Zip file not found at {ZIP_FILE}. This means the download failed. Check API key/rules acceptance.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR during unzipping: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee4e3a",
   "metadata": {},
   "source": [
    "# Block 2: Query and save Brazil Central Bank inflation and interest rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33e94eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory check complete: /content/drive/MyDrive/Project_01\n",
      "✅ Placeholder CSV created and saved at: /content/drive/MyDrive/Project_01/brasil_macro_data.csv\n",
      "You can now safely run the loading block (Block 2).\n"
     ]
    }
   ],
   "source": [
    "# Generate and Save Placeholder CSV\n",
    "\n",
    "# 1. Define the directory path (ensure the folder exists in your Google Drive)\n",
    "DRIVE_BASE_PATH = '/content/drive/MyDrive/Project_01'\n",
    "MACRO_DATA_PATH = os.path.join(DRIVE_BASE_PATH, 'brasil_macro_data.csv')\n",
    "\n",
    "# 2. Create the directory\n",
    "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
    "print(f\"Directory check complete: {DRIVE_BASE_PATH}\")\n",
    "\n",
    "# 3. Create a simple, empty DataFrame with the expected columns/index for now\n",
    "placeholder_data = {\n",
    "    'date': pd.to_datetime(['2023-01', '2023-02']),\n",
    "    'SELIC': [13.75, 13.65], \n",
    "    'IPCA': [5.79, 5.60] \n",
    "}\n",
    "df_placeholder = pd.DataFrame(placeholder_data)\n",
    "df_placeholder.set_index('date', inplace=True)\n",
    "\n",
    "# 4. Save the placeholder DataFrame to the expected path\n",
    "df_placeholder.to_csv(MACRO_DATA_PATH)\n",
    "\n",
    "print(f\"✅ Placeholder CSV created and saved at: {MACRO_DATA_PATH}\")\n",
    "print(\"You can now safely run the loading block (Block 2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e09c95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant for the BCB API limitation\n",
    "BCB_WINDOW_YEARS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9c435ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Brazil Central Bank API\n",
    "\n",
    "def query_bc(serie_code, START_DATE_STR, END_DATE_STR):\n",
    "    \"\"\"\n",
    "    Auxiliary function to query the BCB API directly via URL for a specific time chunk.\n",
    "    Dates must be in DD/MM/YYYY format as required by the BCB API.\n",
    "    \"\"\"\n",
    "    url = f'https://api.bcb.gov.br/dados/serie/bcdata.sgs.{serie_code}/dados?formato=json&dataInicial={START_DATE_STR}&dataFinal={END_DATE_STR}'\n",
    "    \n",
    "    try:\n",
    "        # BCB returns 'data' and 'valor' columns\n",
    "        df = pd.read_json(url)\n",
    "        \n",
    "        if df.empty:\n",
    "             print(f\"Warning: BCB returned empty data for series {serie_code} from {START_DATE_STR} to {END_DATE_STR}.\")\n",
    "             return pd.DataFrame()\n",
    "             \n",
    "        df['date'] = pd.to_datetime(df['data'], dayfirst=True)\n",
    "        df = df.rename(columns={'valor': str(serie_code)})\n",
    "        df.set_index('date', inplace=True)\n",
    "        df = df[[str(serie_code)]] \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying BCB series {serie_code} from {START_DATE_STR} to {END_DATE_STR}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec39c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Function that query the data from Brazil Central Bank API, with a 10 year windown condition for querying\n",
    "\n",
    "def get_series_with_pagination(serie_code, global_start_date_str, global_end_date_str):\n",
    "    \"\"\"\n",
    "    Breaks the total time period into 10-year chunks and calls the BCB API for each chunk.\n",
    "    This bypasses the API's 10-year limit. Input dates must be in DD/MM/YYYY format.\n",
    "    \"\"\"\n",
    "    # Ensure all dates are the FIRST day of their respective month for safer querying\n",
    "    global_start_date = datetime.strptime(global_start_date_str, '%d/%m/%Y').replace(day=1)\n",
    "    global_end_date = datetime.strptime(global_end_date_str, '%d/%m/%Y').replace(day=1)\n",
    "    \n",
    "    all_data = []\n",
    "    current_end_date = global_end_date\n",
    "    \n",
    "    print(f\"Starting pagination for series {serie_code}. Total range: {global_start_date_str} to {global_end_date_str}\")\n",
    "    \n",
    "    while current_end_date >= global_start_date:\n",
    "        # Calculate start date 10 years earlier using relativedelta\n",
    "        current_start_date = current_end_date - relativedelta(years=BCB_WINDOW_YEARS)\n",
    "        \n",
    "        # Ensure that the start date of the chunk is the 1st day of the month\n",
    "        current_start_date = current_start_date.replace(day=1) \n",
    "        \n",
    "        # Ensure the chunk doesn't start before the user's global start date\n",
    "        if current_start_date < global_start_date:\n",
    "            current_start_date = global_start_date\n",
    "            \n",
    "        # Format dates for the query (DD/MM/YYYY)\n",
    "        query_start_date_str = current_start_date.strftime('%d/%m/%Y')\n",
    "        query_end_date_str = current_end_date.strftime('%d/%m/%Y')\n",
    "        \n",
    "        print(f\"  -> Querying chunk: {query_start_date_str} to {query_end_date_str}\")\n",
    "        \n",
    "        df_chunk = query_bc(serie_code, query_start_date_str, query_end_date_str)\n",
    "        \n",
    "        if not df_chunk.empty:\n",
    "            all_data.append(df_chunk)\n",
    "        \n",
    "        # Update the end date for the next iteration: move back one day from the start date of the current chunk\n",
    "        current_end_date = current_start_date - relativedelta(days=1)\n",
    "        \n",
    "        # Stop the loop if we've processed the earliest required data block\n",
    "        if current_start_date == global_start_date:\n",
    "            break\n",
    "\n",
    "    if not all_data:\n",
    "        print(f\"The query returned no data for series {serie_code}.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Concatenate all DataFrames, sort by date, and remove any duplicates\n",
    "    df_final = pd.concat(all_data).sort_index()\n",
    "    df_final = df_final[~df_final.index.duplicated(keep='first')]\n",
    "    print(f\"Pagination successful. Total data points: {len(df_final)}\")\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8bac1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring SELIC data...\n",
      "Starting pagination for series 432. Total range: 01/01/2015 to 01/11/2025\n",
      "  -> Querying chunk: 01/11/2015 to 01/11/2025\n",
      "❌ Error querying BCB series 432 from 01/11/2015 to 01/11/2025: Expected object or value\n",
      "  -> Querying chunk: 01/01/2015 to 31/10/2015\n",
      "Pagination successful. Total data points: 304\n",
      "✅ SELIC Data acquired and paginated successfully.\n",
      "SELIC rows after resampling: 10\n"
     ]
    }
   ],
   "source": [
    "# Main Macro Data Acquisition (with Pagination)\n",
    "\n",
    "# --- USER INPUT ---\n",
    "# Define the global start and end date for the macro data acquisition (DD/MM/YYYY format)\n",
    "GLOBAL_START_DATE_STR = '01/01/2015' \n",
    "GLOBAL_END_DATE_STR = '01/11/2025'   \n",
    "# ------------------\n",
    "\n",
    "# 1. Acquire SELIC Rate (BCB Series 432) using Pagination\n",
    "print(\"Acquiring SELIC data...\")\n",
    "try:\n",
    "    # Use the pagination function to bypass the 10-year limit\n",
    "    df_selic = get_series_with_pagination(\n",
    "        432, \n",
    "        GLOBAL_START_DATE_STR, \n",
    "        GLOBAL_END_DATE_STR\n",
    "    )\n",
    "    df_selic = df_selic.resample('ME').last()\n",
    "    df_selic.index = df_selic.index.to_period('M') \n",
    "    df_selic.rename(columns={'432': 'SELIC'}, inplace=True)\n",
    "    print(\"✅ SELIC Data acquired and paginated successfully.\")\n",
    "    print(f\"SELIC rows after resampling: {len(df_selic)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR processing SELIC data: {e}\")\n",
    "    df_selic = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3014a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acquiring IPCA data...\n",
      "Starting pagination for series 433. Total range: 01/01/2015 to 01/11/2025\n",
      "  -> Querying chunk: 01/11/2015 to 01/11/2025\n",
      "  -> Querying chunk: 01/01/2015 to 31/10/2015\n",
      "Pagination successful. Total data points: 130\n",
      "✅ IPCA Data acquired successfully.\n"
     ]
    }
   ],
   "source": [
    "# Acquire IPCA (Brazilian Inflation Index - Ipeadata)\n",
    "print(\"\\nAcquiring IPCA data...\")\n",
    "try:\n",
    "    # Convert the end date string for the IPCA year filter\n",
    "    global_end_date_dt = datetime.strptime(GLOBAL_END_DATE_STR, '%d/%m/%Y')\n",
    "    \n",
    "    # IPCA Code GMEC12_IPCA12 (12-month accumulated)\n",
    "    df_ipca = get_series_with_pagination(\n",
    "        433, \n",
    "        GLOBAL_START_DATE_STR, \n",
    "        GLOBAL_END_DATE_STR\n",
    "    )\n",
    "    df_ipca.index = df_ipca.index.to_period('M') \n",
    "    df_ipca.rename(columns={'433': 'IPCA'}, inplace=True)\n",
    "    print(\"✅ IPCA Data acquired successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error acquiring IPCA data from Ipeadata: {e}\")\n",
    "    df_ipca = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdd9dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Macro Data Merged and Saved:\n",
      "         IPCA  SELIC\n",
      "date                \n",
      "2015-01  1.24  12.25\n",
      "2015-02  1.22  12.25\n",
      "2015-03  1.32  12.75\n",
      "2015-04  0.71  13.25\n",
      "2015-05  0.74  13.25\n",
      "Macro Data Shape: (130, 2)\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets, clean, and save\n",
    "if not df_selic.empty and not df_ipca.empty:\n",
    "    df_macro = df_ipca.join(df_selic, how='outer') \n",
    "    df_macro = df_macro.ffill()\n",
    "    \n",
    "    # Save the data to Google Drive\n",
    "    df_macro.to_csv(MACRO_DATA_PATH)\n",
    "    \n",
    "    print(\"\\n✅ Final Macro Data Merged and Saved:\")\n",
    "    print(df_macro.head())\n",
    "    print(f\"Macro Data Shape: {df_macro.shape}\")\n",
    "else:\n",
    "    print(\"\\n❌ Could not merge data due to empty SELIC or IPCA datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a268f",
   "metadata": {},
   "source": [
    "# Simulate time variable for Credit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73d81d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load Micro Data from: /content/drive/MyDrive/Project_01/data/raw/kaggle/home-credit-default-risk/application_train.csv\n",
      "✅ Micro Data loaded and Time Index simulated successfully.\n",
      "   SK_ID_CURR  TARGET TIME_INDEX\n",
      "0      100002       1    2017-01\n",
      "1      100003       0    2014-12\n",
      "2      100004       0    2014-04\n",
      "3      100006       0    2018-02\n",
      "4      100007       0    2016-05\n",
      "\n",
      "Micro Data Shape: (307511, 123)\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the main training data\n",
    "RAW_TRAIN_FILE = os.path.join(DATA_DIR_RAW, 'application_train.csv')\n",
    "\n",
    "print(f\"Attempting to load Micro Data from: {RAW_TRAIN_FILE}\")\n",
    "\n",
    "try:\n",
    "    df_train = pd.read_csv(RAW_TRAIN_FILE)\n",
    "    \n",
    "    # 1. Simulate the Time Variable (Crucial for Macro Merge)\n",
    "    # The Home Credit data is cross-sectional (no monthly date). \n",
    "    # We must assign a random monthly period to each client \n",
    "    # for the purpose of joining with the monthly macro data (df_macro).\n",
    "    \n",
    "    # Range of dates for simulation (e.g., last 5 years relative to the data creation)\n",
    "    # The actual date of the dataset is not critical, only the relative time index.\n",
    "    start_date = datetime(2013, 1, 1)\n",
    "    end_date = datetime(2018, 5, 1) # Kaggle data was originally published around this time\n",
    "    \n",
    "    date_range = pd.period_range(start=start_date, end=end_date, freq='M')\n",
    "    \n",
    "    # Assign a random date from the range to each client\n",
    "    df_train['TIME_INDEX'] = random.choices(date_range, k=len(df_train))\n",
    "    df_train['TIME_INDEX'] = df_train['TIME_INDEX'].astype('object')\n",
    "    \n",
    "    # 2. Convert to Period Index for Joining\n",
    "    df_train['TIME_INDEX'] = df_train['TIME_INDEX'].apply(lambda x: pd.Period(x, freq='M'))\n",
    "    \n",
    "    print(\"✅ Micro Data loaded and Time Index simulated successfully.\")\n",
    "    print(df_train[['SK_ID_CURR', 'TARGET', 'TIME_INDEX']].head())\n",
    "    print(\"\\nMicro Data Shape:\", df_train.shape)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: File not found at {RAW_TRAIN_FILE}. The Kaggle download or unzipping may have failed.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during micro data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b59fe",
   "metadata": {},
   "source": [
    "# Load Macro Data and Prepare Time Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e6f21511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load Macro Data from: /content/drive/MyDrive/Project_01/brasil_macro_data.csv\n",
      "✅ Macro Data loaded and Time Index prepared successfully.\n",
      "            IPCA  SELIC\n",
      "TIME_INDEX             \n",
      "2015-01     1.24  12.25\n",
      "2015-02     1.22  12.25\n",
      "2015-03     1.32  12.75\n",
      "2015-04     0.71  13.25\n",
      "2015-05     0.74  13.25\n",
      "\n",
      "Macro Data Shape: (130, 2)\n"
     ]
    }
   ],
   "source": [
    "# The path to the macro data file was defined in Block 1 (e.g., .../Project_01/brasil_macro_data.csv)\n",
    "print(f\"Attempting to load Macro Data from: {MACRO_DATA_PATH}\")\n",
    "\n",
    "try:\n",
    "    df_macro = pd.read_csv(MACRO_DATA_PATH)\n",
    "    \n",
    "    # 1. Convert the Date Column to Datetime\n",
    "    # We assume the time column is named 'DATE' (or 'date', depending on the API output/saving convention)\n",
    "    # Adjust 'DATE' if your CSV uses a different column name (e.g., 'date')\n",
    "    if 'DATE' in df_macro.columns:\n",
    "        df_macro['DATE'] = pd.to_datetime(df_macro['DATE'])\n",
    "    elif 'date' in df_macro.columns:\n",
    "        df_macro['DATE'] = pd.to_datetime(df_macro['date'])\n",
    "        df_macro = df_macro.drop(columns=['date']) # Clean up the old column if it was renamed\n",
    "    else:\n",
    "        raise ValueError(\"Time column ('DATE' or 'date') not found in macro data.\")\n",
    "\n",
    "\n",
    "    # 2. Convert to Period Index (Monthly Frequency - 'M')\n",
    "    # This conversion is fundamental for merging with df_train['TIME_INDEX']\n",
    "    df_macro['TIME_INDEX'] = df_macro['DATE'].dt.to_period('M')\n",
    "    \n",
    "    # 3. Drop the original datetime column\n",
    "    df_macro = df_macro.drop(columns=['DATE'])\n",
    "    \n",
    "    # Set TIME_INDEX as the index for easier joining (optional, but good practice)\n",
    "    df_macro = df_macro.set_index('TIME_INDEX')\n",
    "\n",
    "    # Display the result\n",
    "    print(\"✅ Macro Data loaded and Time Index prepared successfully.\")\n",
    "    print(df_macro.head())\n",
    "    print(\"\\nMacro Data Shape:\", df_macro.shape)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Macro file not found at {MACRO_DATA_PATH}. Check if the file is in your Google Drive.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during macro data loading or processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2f593",
   "metadata": {},
   "source": [
    "## Merging Macro and Micro home credit loan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0beee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting merge of micro (df_train) and macro (df_macro) data...\n",
      "✅ Datasets merged successfully.\n",
      "\n",
      "Enriched Data Sample (with new macro features):\n",
      "   SK_ID_CURR  TARGET TIME_INDEX  IPCA  SELIC\n",
      "0      100002       1    2017-01  0.38  14.25\n",
      "1      100003       0    2014-12   NaN    NaN\n",
      "2      100004       0    2014-04   NaN    NaN\n",
      "3      100006       0    2018-02  0.32  14.25\n",
      "4      100007       0    2016-05  0.78  14.25\n",
      "\n",
      "Final Enriched Shape: (307511, 125)\n"
     ]
    }
   ],
   "source": [
    "# Block 7: Merge Micro and Macro Data\n",
    "\n",
    "print(\"Starting merge of micro (df_train) and macro (df_macro) data...\")\n",
    "\n",
    "# The merge key is the TIME_INDEX (Period('M'))\n",
    "# We use a LEFT JOIN to keep all client rows from df_train and pull in the macro data.\n",
    "try:\n",
    "    # Ensure df_macro is set up for joining by index.\n",
    "    # The previous steps should have set df_macro.index to TIME_INDEX (Period('M')).\n",
    "    df_train_enriched = df_train.merge(\n",
    "        df_macro, \n",
    "        left_on='TIME_INDEX', \n",
    "        right_index=True,  # Merge df_macro based on its index (TIME_INDEX)\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Check for successful join\n",
    "    if df_train_enriched.isnull().any().any():\n",
    "        # Check if only the new macro columns have NaNs (expected for early dates)\n",
    "        new_cols = df_macro.columns\n",
    "        if df_train_enriched[new_cols].isnull().all().all():\n",
    "            print(\"⚠️ Warning: New macro columns contain NaNs. This is expected if the IPCA data did not cover the full range of TIME_INDEX simulation (e.g., prior to 2000).\")\n",
    "            \n",
    "    print(\"✅ Datasets merged successfully.\")\n",
    "    print(\"\\nEnriched Data Sample (with new macro features):\")\n",
    "    # Displaying the target and the newly merged macro features\n",
    "    print(df_train_enriched[['SK_ID_CURR', 'TARGET', 'TIME_INDEX', 'IPCA', 'SELIC']].head())\n",
    "    print(f\"\\nFinal Enriched Shape: {df_train_enriched.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during the merge process: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
