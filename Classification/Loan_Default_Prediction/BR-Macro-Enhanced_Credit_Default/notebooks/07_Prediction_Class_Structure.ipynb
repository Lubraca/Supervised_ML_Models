{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7f60db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.2)\n",
      "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
      "Downloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: category_encoders\n",
      "Successfully installed category_encoders-2.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install category_encoders if not already installed in the environment\n",
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e419124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from category_encoders import TargetEncoder\n",
    "from typing import Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8612c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully added 'src' directory to Python path.\n"
     ]
    }
   ],
   "source": [
    "# Define project Path in Colab\n",
    "PROJECT_BASE_PATH = '/content/drive/MyDrive/Project_01' \n",
    "\n",
    "# ADD 'src' DIRECTORY TO PYTHON PATH\n",
    "SRC_PATH = os.path.join(PROJECT_BASE_PATH, 'src')\n",
    "\n",
    "# verify if SRC_PATH is already in sys.path\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "    print(\"‚úÖ Successfully added 'src' directory to Python path.\")\n",
    "\n",
    "# IMPORT Paths CLASS FROM config MODULE\n",
    "from config import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4800f3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Project configuration (Paths) initialized successfully.\n",
      "Raw Data Path check: /content/drive/MyDrive/Project_01/data/raw/application_train.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from config import Paths\n",
    "    \n",
    "    cfg = Paths(PROJECT_BASE_PATH)\n",
    "    cfg.create_dirs() \n",
    "    \n",
    "    print(\"\\n‚úÖ Project configuration (Paths) initialized successfully.\")\n",
    "    print(f\"Raw Data Path check: {cfg.TRAIN_RAW_FILE}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå Error: Could not import Paths from config module.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c03711",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PROCESSED = os.path.join(PROJECT_BASE_PATH, 'data', 'processed')\n",
    "TRAIN_PROCESSED_FILE = os.path.join(DATA_DIR_PROCESSED, 'train_enriched.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6daac310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded final training data. Shape: (307511, 125)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # --- LOAD TRAIN DATA USING CONFIG PATHS ---\n",
    "    df_train_final = pd.read_csv(cfg.TRAIN_PROCESSED_FILE)\n",
    "    \n",
    "    # ESSENTIAL FIX: Ensures TIME_INDEX is of temporal type after loading from CSV\n",
    "    df_train_final['TIME_INDEX'] = pd.to_datetime(df_train_final['TIME_INDEX'])\n",
    "    \n",
    "    print(f\"‚úÖ Loaded final training data. Shape: {df_train_final.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: Final processed file not found. Check your PROJECT_BASE_PATH.\")\n",
    "    df_train_final = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f469b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Feature Engineering ---\n",
      "‚úÖ TRAIN: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\n",
      "‚úÖ TRAIN: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\n",
      "\n",
      "Training set shape after Macro Feature Engineering: (307511, 133)\n"
     ]
    }
   ],
   "source": [
    "if df_train_final is not None:\n",
    "    print(\"--- Starting Feature Engineering ---\")\n",
    "    \n",
    "    # --- 1. Create Lagged and Dynamic Macro Features (TRAIN SET) ---\n",
    "    \n",
    "    # KeyError/SetIndex Fix: Ensures TIME_INDEX is a regular column before setting\n",
    "    if 'TIME_INDEX' not in df_train_final.columns and 'TIME_INDEX' in df_train_final.index.names:\n",
    "        df_train_final = df_train_final.reset_index(level='TIME_INDEX')\n",
    "        print(\"üí° TRAIN: TIME_INDEX restored from index to column.\")\n",
    "    \n",
    "    # Temporarily set TIME_INDEX as the index for time-series operations\n",
    "    df_train_final = df_train_final.set_index('TIME_INDEX')\n",
    "    \n",
    "    # Use the correct column names\n",
    "    macro_features_to_engineer = ['SELIC', 'IPCA'] \n",
    "    \n",
    "    for col in macro_features_to_engineer:\n",
    "        # A) Lag (Previous Month's Value)\n",
    "        df_train_final[f'{col}_LAG1'] = df_train_final.groupby('SK_ID_CURR')[col].shift(1)\n",
    "\n",
    "        # B) Change (Current Month - Previous Month)\n",
    "        df_train_final[f'{col}_CHANGE'] = df_train_final[col] - df_train_final[f'{col}_LAG1']\n",
    "        \n",
    "        # C) Trend (3-Month Rolling Mean)\n",
    "        df_train_final[f'{col}_ROLLING_MEAN3'] = df_train_final.groupby('SK_ID_CURR')[col].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        \n",
    "    print(f\"‚úÖ TRAIN: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\")\n",
    " # --- 2. Temporal Features from TIME_INDEX (TRAIN SET) ---\n",
    "    \n",
    "    # Extract month and year using the already defined index\n",
    "    df_train_final['MONTH_OF_YEAR'] = df_train_final.index.to_series().dt.month\n",
    "    df_train_final['YEAR'] = df_train_final.index.to_series().dt.year\n",
    "    \n",
    "    print(\"‚úÖ TRAIN: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\")\n",
    "\n",
    "    # Restore TIME_INDEX as a regular column\n",
    "    df_train_final = df_train_final.reset_index()\n",
    "\n",
    "    print(f\"\\nTraining set shape after Macro Feature Engineering: {df_train_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35fdf5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3039.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3038.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307506</th>\n",
       "      <td>-236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307507</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307508</th>\n",
       "      <td>-7921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307509</th>\n",
       "      <td>-4786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307510</th>\n",
       "      <td>-1262.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307511 rows √ó 1 columns</p>\n",
       "</div><br><label><b>dtype:</b> float64</label>"
      ],
      "text/plain": [
       "0         -637.0\n",
       "1        -1188.0\n",
       "2         -225.0\n",
       "3        -3039.0\n",
       "4        -3038.0\n",
       "           ...  \n",
       "307506    -236.0\n",
       "307507       NaN\n",
       "307508   -7921.0\n",
       "307509   -4786.0\n",
       "307510   -1262.0\n",
       "Name: DAYS_EMPLOYED, Length: 307511, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Fix the DAYS_EMPLOYED Anomaly\n",
    "# Replace the extreme positive value with NaN for proper imputation later.\n",
    "DAYS_EMPLOYED_ANOMALY = 365243 \n",
    "df_train_final['DAYS_EMPLOYED'].replace({DAYS_EMPLOYED_ANOMALY: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "324e7bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 3 core ratio features.\n"
     ]
    }
   ],
   "source": [
    "# 2. Create Simple Ratio Features (Crucial for risk assessment)\n",
    "# These are highly predictive and do not increase dimensionality.\n",
    "df_train_final['CREDIT_INCOME_RATIO'] = df_train_final['AMT_CREDIT'] / df_train_final['AMT_INCOME_TOTAL']\n",
    "\n",
    "df_train_final['ANNUITY_INCOME_RATIO'] = df_train_final['AMT_ANNUITY'] / df_train_final['AMT_INCOME_TOTAL']\n",
    "\n",
    "df_train_final['PAYMENT_RATE'] = df_train_final['AMT_ANNUITY'] / df_train_final['AMT_CREDIT']\n",
    "\n",
    "print(\"‚úÖ Created 3 core ratio features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3138761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TIME_INDEX converted to numerical YEAR and original index removed.\n",
      "\n",
      "Training set shape after revised feature engineering: (307511, 135)\n"
     ]
    }
   ],
   "source": [
    "if 'TIME_INDEX' in df_train_final.columns:\n",
    "    # Convert 'YYYY-MM' string to YYYY * 12 + MM, or simply the Year (simple numerical feature)\n",
    "    try:\n",
    "        df_train_final['YEAR'] = pd.to_datetime(df_train_final['TIME_INDEX']).dt.year\n",
    "        \n",
    "        # Drop the original string TIME_INDEX to prevent errors in Block 13\n",
    "        df_train_final = df_train_final.drop(columns=['TIME_INDEX'])\n",
    "        \n",
    "        print(\"‚úÖ TIME_INDEX converted to numerical YEAR and original index removed.\")\n",
    "    except Exception:\n",
    "        print(\"‚ùå WARNING: Could not convert TIME_INDEX to datetime. Dropping TIME_INDEX.\")\n",
    "        df_train_final = df_train_final.drop(columns=['TIME_INDEX'])\n",
    "\n",
    "\n",
    "print(f\"\\nTraining set shape after revised feature engineering: {df_train_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96348686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataFrame created successfully and saved at REPORT_DIR as data_train_schema.csv\n"
     ]
    }
   ],
   "source": [
    "# creation of a schema for schemas.py in order to deploy an endpoint for the project\n",
    "df_train_schema = pd.DataFrame(\n",
    "    df_train_final.dtypes, \n",
    "    columns = [\"data_type\"]\n",
    "    )\n",
    "df_train_schema = df_train_schema.reset_index()\n",
    "df_train_schema = df_train_schema.rename(columns={'index': 'feature_name'})\n",
    "\n",
    "df_train_schema.to_csv(os.path.join(cfg.REPORT_DIR, \"data_train_schema.csv\"))\n",
    "\n",
    "print(\"‚úÖ DataFrame created successfully and saved at REPORT_DIR as data_train_schema.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022f0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting: Saving Preprocessing Artifacts ---\n",
      "‚úÖ Artifact 1 (Imputation Map) saved to: /content/drive/MyDrive/Project_01/models/imputation_means_map.pkl\n",
      "‚úÖ Artifact 2 (Target Encoder Map) saved to: /content/drive/MyDrive/Project_01/models/final_target_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting: Saving Preprocessing Artifacts ---\")\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "EXCLUDED_COLS = ['SK_ID_CURR', 'TARGET'] \n",
    "y_train = df_train_final['TARGET']\n",
    "\n",
    "\n",
    "# --- 2. Artifact 1: Saving Imputation Means Map ---\n",
    "\n",
    "imputation_maps = {}\n",
    "# Select all columns that can hold numerical data (float/int)\n",
    "numerical_cols = df_train_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if col in EXCLUDED_COLS:\n",
    "        continue\n",
    "    # Calculate the mean after replacing Inf with NaN (robust method)\n",
    "    imputation_maps[col] = df_train_final[col].replace([np.inf, -np.inf], np.nan).mean()\n",
    "\n",
    "# Add the critical anomaly value for DAYS_EMPLOYED\n",
    "imputation_maps['DAYS_EMPLOYED_ANOMALY'] = 365243 \n",
    "\n",
    "# Construct path using os.path.join for cross-platform compatibility\n",
    "imputation_map_path = os.path.join(cfg.MODEL_DIR, 'imputation_means_map.pkl')\n",
    "joblib.dump(imputation_maps, imputation_map_path)\n",
    "print(f\"‚úÖ Artifact 1 (Imputation Map) saved to: {imputation_map_path}\")\n",
    "\n",
    "\n",
    "# --- 3. Artifact 2: Saving the TargetEncoder Map ---\n",
    "\n",
    "# Identify the categorical features used in Block 13\n",
    "categorical_features = df_train_final.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Fit a FINAL TargetEncoder on the entire training set (No CV this time!)\n",
    "final_target_encoder = TargetEncoder(cols=categorical_features)\n",
    "final_target_encoder.fit(df_train_final[categorical_features], y_train)\n",
    "\n",
    "# Construct path using os.path.join\n",
    "target_encoder_path = os.path.join(cfg.MODEL_DIR, 'final_target_encoder.pkl')\n",
    "joblib.dump(final_target_encoder, target_encoder_path)\n",
    "print(f\"‚úÖ Artifact 2 (Target Encoder Map) saved to: {target_encoder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcafefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PredictionHandler:\n",
    "    \"\"\"\n",
    "    Handles loading MLOps artifacts and making consistent predictions \n",
    "    for single or batch inputs in a live environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load All Artifacts in __init__\n",
    "    def __init__(self, model_path: str, mean_map_path: str, encoder_path: str):\n",
    "        \n",
    "        try:\n",
    "            # Load MLOps Artifacts\n",
    "            self.model = joblib.load(model_path)\n",
    "            self.imputation_maps = joblib.load(mean_map_path)   # Loads saved means/anomaly\n",
    "            self.target_encoder = joblib.load(encoder_path)     # Loads saved encoder object\n",
    "            \n",
    "            # The list of feature names used during training is CRITICAL for alignment\n",
    "            self.feature_names = list(self.model.feature_name_)\n",
    "            print(\"‚úÖ PredictionHandler fully initialized with all MLOps artifacts.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR: Failed to load MLOps artifacts: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "        \n",
    "    def _clean_names(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Removes problematic characters for LightGBM/XGBoost compatibility.\"\"\"\n",
    "        cols = df.columns\n",
    "        new_cols = []\n",
    "        for col in cols:\n",
    "            # Regex to keep only alphanumeric characters and underscores\n",
    "            new_col = re.sub(r'[^A-Za-z0-9_]+', '', col)\n",
    "            new_cols.append(new_col)\n",
    "        df.columns = new_cols\n",
    "        return df\n",
    "\n",
    "\n",
    "    def preprocess(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Applies the entire sequential preprocessing pipeline consistently.\"\"\"\n",
    "        \n",
    "        df = input_df.copy()\n",
    "\n",
    "        # --- A. (Feature Engineering) ---\n",
    "        \n",
    "        # 1. Anomaly Fix (Uses saved value from imputation map)\n",
    "        if 'DAYS_EMPLOYED' in df.columns:\n",
    "            # Load the saved value for the anomaly fix (365243)\n",
    "            DAYS_EMPLOYED_ANOMALY = self.imputation_maps.get('DAYS_EMPLOYED_ANOMALY', 365243)\n",
    "            df['DAYS_EMPLOYED'].replace(DAYS_EMPLOYED_ANOMALY, np.nan, inplace=True)\n",
    "            df['DAYS_EMPLOYED'] = np.abs(df['DAYS_EMPLOYED'])\n",
    "        \n",
    "        # 2. Ratio Features (Must match training!)\n",
    "        df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "        df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "        \n",
    "        # 3. Time Feature Conversion (Must match training!)\n",
    "        if 'TIME_INDEX' in df.columns:\n",
    "             try:\n",
    "                 df['YEAR'] = pd.to_datetime(df['TIME_INDEX']).dt.year\n",
    "                 df = df.drop(columns=['TIME_INDEX'])\n",
    "             except:\n",
    "                 pass\n",
    "                \n",
    "        # --- B. (Target Encoding) ---\n",
    "        \n",
    "        # Apply the SAVED encoder to the categorical columns of the live data\n",
    "        df = self.target_encoder.transform(df)\n",
    "        \n",
    "        # Drop original categorical columns (they are now encoded)\n",
    "        df = df.drop(columns=self.target_encoder.cols, errors='ignore') \n",
    "        \n",
    "        # --- C. Final Cleaning and Alignment ---\n",
    "        \n",
    "        # 1. Clean Feature Names\n",
    "        df = self._clean_names(df)\n",
    "\n",
    "        # 2. Handle NaN/Inf using SAVED MEANS (CORRECT MLOPS IMPUTATION)\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Fill missing values using the means calculated from the TRAINING data\n",
    "        for col, mean_val in self.imputation_maps.items():\n",
    "            if col in df.columns and col != 'DAYS_EMPLOYED_ANOMALY': \n",
    "                df[col] = df[col].fillna(mean_val)\n",
    "        \n",
    "        # 3. Align Columns (CRITICAL MLOps Step)\n",
    "        \n",
    "        # Select and re-order columns to match the model's training list\n",
    "        processed_df = df[[col for col in self.feature_names if col in df.columns]]\n",
    "        # Fill any missing engineered features (that weren't in the raw input) with 0 or a consistent value\n",
    "        processed_df = processed_df.reindex(columns=self.feature_names, fill_value=0) \n",
    "        \n",
    "        return processed_df\n",
    "\n",
    "\n",
    "    def predict_proba(self, raw_input_data: Dict[str, Any]) -> float:\n",
    "        \"\"\"\n",
    "        Receives raw input data (e.g., from a JSON API request) and returns the \n",
    "        probability of default (Target=1).\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            return 0.5 # Default prediction if model failed to load\n",
    "\n",
    "        # 1. Convert input dictionary/JSON to DataFrame\n",
    "        input_df = pd.DataFrame([raw_input_data])\n",
    "        \n",
    "        # 2. Preprocess the data\n",
    "        processed_df = self.preprocess(input_df)\n",
    "        \n",
    "        # 3. Ensure column order matches the training data (CRITICAL!)\n",
    "        # The reindex in preprocess should handle this, but an explicit check is safe:\n",
    "        # processed_df = processed_df[self.feature_names] \n",
    "\n",
    "        # 4. Generate prediction probability\n",
    "        prediction_proba = self.model.predict_proba(processed_df)[0][1]\n",
    "        \n",
    "        return prediction_proba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
