{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2692ba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.2)\n",
      "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
      "Downloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: category_encoders\n",
      "Successfully installed category_encoders-2.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install category_encoders if not already installed in the environment\n",
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d0b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully added 'src' directory to Python path.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define project Path in Colab\n",
    "PROJECT_BASE_PATH = '/content/drive/MyDrive/Project_01' \n",
    "\n",
    "# ADD 'src' DIRECTORY TO PYTHON PATH\n",
    "SRC_PATH = os.path.join(PROJECT_BASE_PATH, 'src')\n",
    "\n",
    "# verify if SRC_PATH is already in sys.path\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "    print(\"‚úÖ Successfully added 'src' directory to Python path.\")\n",
    "\n",
    "# IMPORT Paths CLASS FROM config MODULE\n",
    "from config import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8182dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Project configuration (Paths) initialized successfully.\n",
      "Raw Data Path check: /content/drive/MyDrive/Project_01/data/raw/application_train.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from config import Paths\n",
    "    \n",
    "    # 3. Inicialize a inst√¢ncia com um nome √∫nico (cfg)\n",
    "    cfg = Paths(PROJECT_BASE_PATH) # <-- Mudan√ßa aqui\n",
    "    cfg.create_dirs() \n",
    "    \n",
    "    print(\"\\n‚úÖ Project configuration (Paths) initialized successfully.\")\n",
    "    print(f\"Raw Data Path check: {cfg.TRAIN_RAW_FILE}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå Error: Could not import Paths from config module.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92cf6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PROCESSED = os.path.join(PROJECT_BASE_PATH, 'data', 'processed')\n",
    "TRAIN_PROCESSED_FILE = os.path.join(DATA_DIR_PROCESSED, 'train_enriched.csv')\n",
    "TEST_PROCESSED_FILE = os.path.join(DATA_DIR_PROCESSED, 'test_enriched.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68db1350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded final training data. Shape: (307511, 125)\n",
      "‚úÖ Loaded final testing data. Shape: (48744, 124)\n",
      "--- Starting Feature Engineering ---\n",
      "‚úÖ TRAIN: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\n",
      "‚úÖ TRAIN: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\n",
      "\n",
      "Training set shape after Macro Feature Engineering: (307511, 133)\n",
      "‚úÖ TEST: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\n",
      "‚úÖ TEST: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\n",
      "\n",
      "Testing set shape after Macro Feature Engineering: (48744, 132)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # --- LOAD TRAIN DATA USING CONFIG PATHS ---\n",
    "    df_train_final = pd.read_csv(cfg.TRAIN_PROCESSED_FILE)\n",
    "    \n",
    "    # ESSENTIAL FIX: Ensures TIME_INDEX is of temporal type after loading from CSV\n",
    "    df_train_final['TIME_INDEX'] = pd.to_datetime(df_train_final['TIME_INDEX'])\n",
    "    \n",
    "    print(f\"‚úÖ Loaded final training data. Shape: {df_train_final.shape}\")\n",
    "\n",
    "    # --- LOAD TEST DATA USING CONFIG PATHS ---\n",
    "    df_test_final = pd.read_csv(cfg.TEST_PROCESSED_FILE)\n",
    "    \n",
    "    # Apply the same temporal conversion to the Test Set\n",
    "    df_test_final['TIME_INDEX'] = pd.to_datetime(df_test_final['TIME_INDEX'])\n",
    "    \n",
    "    print(f\"‚úÖ Loaded final testing data. Shape: {df_test_final.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: Final processed file not found. Check if Block 9 was run and files exist at: {cfg.DATA_PROCESSED_DIR}\")\n",
    "    df_train_final = None\n",
    "    df_test_final = None\n",
    "if df_train_final is not None and df_test_final is not None:\n",
    "    print(\"--- Starting Feature Engineering ---\")\n",
    "    \n",
    "    # --- 1. Create Lagged and Dynamic Macro Features (TRAIN SET) ---\n",
    "    \n",
    "    # KeyError/SetIndex Fix: Ensures TIME_INDEX is a regular column before setting\n",
    "    if 'TIME_INDEX' not in df_train_final.columns and 'TIME_INDEX' in df_train_final.index.names:\n",
    "        df_train_final = df_train_final.reset_index(level='TIME_INDEX')\n",
    "        print(\"üí° TRAIN: TIME_INDEX restored from index to column.\")\n",
    "    \n",
    "    # Temporarily set TIME_INDEX as the index for time-series operations\n",
    "    df_train_final = df_train_final.set_index('TIME_INDEX')\n",
    "    \n",
    "    # Use the correct column names\n",
    "    macro_features_to_engineer = ['SELIC', 'IPCA'] \n",
    "    \n",
    "    for col in macro_features_to_engineer:\n",
    "        # A) Lag (Previous Month's Value)\n",
    "        df_train_final[f'{col}_LAG1'] = df_train_final.groupby('SK_ID_CURR')[col].shift(1)\n",
    "\n",
    "        # B) Change (Current Month - Previous Month)\n",
    "        df_train_final[f'{col}_CHANGE'] = df_train_final[col] - df_train_final[f'{col}_LAG1']\n",
    "        \n",
    "        # C) Trend (3-Month Rolling Mean)\n",
    "        df_train_final[f'{col}_ROLLING_MEAN3'] = df_train_final.groupby('SK_ID_CURR')[col].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        \n",
    "    print(f\"‚úÖ TRAIN: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\")\n",
    "\n",
    "    # --- 2. Temporal Features from TIME_INDEX (TRAIN SET) ---\n",
    "    \n",
    "    # Extract month and year using the already defined index\n",
    "    df_train_final['MONTH_OF_YEAR'] = df_train_final.index.to_series().dt.month\n",
    "    df_train_final['YEAR'] = df_train_final.index.to_series().dt.year\n",
    "    \n",
    "    print(\"‚úÖ TRAIN: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\")\n",
    "\n",
    "    # Restore TIME_INDEX as a regular column\n",
    "    df_train_final = df_train_final.reset_index()\n",
    "\n",
    "    print(f\"\\nTraining set shape after Macro Feature Engineering: {df_train_final.shape}\")\n",
    "    \n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --- REPEAT FOR TEST SET ---\n",
    "    # --------------------------------------------------------------------------------\n",
    "    \n",
    "    # KeyError/SetIndex Fix for the Test Set\n",
    "    if 'TIME_INDEX' not in df_test_final.columns and 'TIME_INDEX' in df_test_final.index.names:\n",
    "        df_test_final = df_test_final.reset_index(level='TIME_INDEX')\n",
    "        print(\"üí° TEST: TIME_INDEX restored from index to column.\")\n",
    "    \n",
    "    # Temporarily set TIME_INDEX as the index for time-series operations\n",
    "    df_test_final = df_test_final.set_index('TIME_INDEX')\n",
    "    \n",
    "    for col in macro_features_to_engineer:\n",
    "        df_test_final[f'{col}_LAG1'] = df_test_final.groupby('SK_ID_CURR')[col].shift(1)\n",
    "        df_test_final[f'{col}_CHANGE'] = df_test_final[col] - df_test_final[f'{col}_LAG1']\n",
    "        df_test_final[f'{col}_ROLLING_MEAN3'] = df_test_final.groupby('SK_ID_CURR')[col].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        \n",
    "    print(f\"‚úÖ TEST: Created dynamic features: SELIC/IPCA LAGs, Changes, and Rolling Means.\")\n",
    "\n",
    "    # Temporal Features (TEST SET)\n",
    "    df_test_final['MONTH_OF_YEAR'] = df_test_final.index.to_series().dt.month\n",
    "    df_test_final['YEAR'] = df_test_final.index.to_series().dt.year\n",
    "    \n",
    "    print(\"‚úÖ TEST: Created cyclical temporal features (MONTH_OF_YEAR, YEAR).\")\n",
    "\n",
    "    # Restore TIME_INDEX as a regular column\n",
    "    df_test_final = df_test_final.reset_index()\n",
    "\n",
    "    print(f\"\\nTesting set shape after Macro Feature Engineering: {df_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4f5c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Micro Feature Engineering ---\n",
      "‚úÖ DAYS_EMPLOYED anomaly fixed and replaced with NaN.\n",
      "‚úÖ Created 3 core ratio features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TIME_INDEX converted to numerical YEAR and original index removed.\n",
      "\n",
      "Training set shape after revised feature engineering: (307511, 135)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Micro Feature Engineering ---\")\n",
    "\n",
    "# Treat DAYS_EMPLOYED anomaly (365243 days ‚âà 1000 years) as missing (NaN) and flag it\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# --- IMPORTANT DATA CLEANING NOTE: DAYS_EMPLOYED ANOMALY ---\n",
    "# The value 365243 in DAYS_EMPLOYED is a known data anomaly specific to the Home Credit dataset.\n",
    "# It represents approximately 1000 years, and is used by the client/bank to code for\n",
    "# applicants who are currently **unemployed** or whose employment status is unverified.\n",
    "#\n",
    "# Best Practice Treatment:\n",
    "# 1. Replace the anomalous value (365243) with **NaN** to treat it as a missing value.\n",
    "# 2. Create a new **binary feature** (DAYS_EMPLOYED_ANOM) to allow the model (like LightGBM)\n",
    "#    to explicitly learn the predictive power of this specific 'unemployed/anomaly' group.\n",
    "# This prevents the model from interpreting 365243 as a literal, extremely long employment history.\n",
    "# -----------------------------------------------------------\n",
    "# --- 1. Anomaly Treatment and Transformation ---\n",
    "\n",
    "# 1. Fix the DAYS_EMPLOYED Anomaly\n",
    "# Replace the extreme positive value with NaN for proper imputation later.\n",
    "DAYS_EMPLOYED_ANOMALY = 365243 \n",
    "df_train_final['DAYS_EMPLOYED'].replace({DAYS_EMPLOYED_ANOMALY: np.nan})\n",
    "df_test_final['DAYS_EMPLOYED'].replace({DAYS_EMPLOYED_ANOMALY: np.nan})\n",
    "print(\"‚úÖ DAYS_EMPLOYED anomaly fixed and replaced with NaN.\")\n",
    "\n",
    "# 2. Create Simple Ratio Features (Crucial for risk assessment)\n",
    "# These are highly predictive and do not increase dimensionality.\n",
    "df_train_final['CREDIT_INCOME_RATIO'] = df_train_final['AMT_CREDIT'] / df_train_final['AMT_INCOME_TOTAL']\n",
    "df_test_final['CREDIT_INCOME_RATIO'] = df_test_final['AMT_CREDIT'] / df_test_final['AMT_INCOME_TOTAL']\n",
    "\n",
    "df_train_final['ANNUITY_INCOME_RATIO'] = df_train_final['AMT_ANNUITY'] / df_train_final['AMT_INCOME_TOTAL']\n",
    "df_test_final['ANNUITY_INCOME_RATIO'] = df_test_final['AMT_ANNUITY'] / df_test_final['AMT_INCOME_TOTAL']\n",
    "\n",
    "df_train_final['PAYMENT_RATE'] = df_train_final['AMT_ANNUITY'] / df_train_final['AMT_CREDIT']\n",
    "df_test_final['PAYMENT_RATE'] = df_test_final['AMT_ANNUITY'] / df_test_final['AMT_CREDIT']\n",
    "\n",
    "print(\"‚úÖ Created 3 core ratio features.\")\n",
    "\n",
    "# Handle TIME_INDEX\n",
    "\n",
    "if 'TIME_INDEX' in df_train_final.columns:\n",
    "    # Convert 'YYYY-MM' string to YYYY * 12 + MM, or simply the Year (simple numerical feature)\n",
    "    try:\n",
    "        df_train_final['YEAR'] = pd.to_datetime(df_train_final['TIME_INDEX']).dt.year\n",
    "        df_test_final['YEAR'] = pd.to_datetime(df_test_final['TIME_INDEX']).dt.year\n",
    "        \n",
    "        # Drop the original string TIME_INDEX to prevent errors in Block 13\n",
    "        df_train_final = df_train_final.drop(columns=['TIME_INDEX'])\n",
    "        df_test_final = df_test_final.drop(columns=['TIME_INDEX'])\n",
    "        \n",
    "        print(\"‚úÖ TIME_INDEX converted to numerical YEAR and original index removed.\")\n",
    "    except Exception:\n",
    "        print(\"‚ùå WARNING: Could not convert TIME_INDEX to datetime. Dropping TIME_INDEX.\")\n",
    "        df_train_final = df_train_final.drop(columns=['TIME_INDEX'])\n",
    "        df_test_final = df_test_final.drop(columns=['TIME_INDEX'])\n",
    "\n",
    "\n",
    "print(f\"\\nTraining set shape after revised feature engineering: {df_train_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807ae5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import re\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed72a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting: K-Fold Target Encoding (Leakage-safe) ---\n",
      "Number of categorical features: 16\n",
      "Categorical features: ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Categorical features list saved to: /content/drive/MyDrive/Project_01/reports/categorical_features.csv\n",
      "  -> Fitting TargetEncoder on fold 1/5...\n",
      "  -> Fitting TargetEncoder on fold 2/5...\n",
      "  -> Fitting TargetEncoder on fold 3/5...\n",
      "  -> Fitting TargetEncoder on fold 4/5...\n",
      "  -> Fitting TargetEncoder on fold 5/5...\n",
      "\n",
      "--- Fitting final TargetEncoder on the full training set (for test + API) ---\n",
      "‚úÖ OOF Target Encoding completed. Encoded TRAIN shape: (307511, 16)\n",
      "‚úÖ TEST Target Encoding completed. Encoded TEST shape: (48744, 16)\n",
      "\n",
      "Shapes after adding encoded features and dropping raw categoricals:\n",
      "  > Train: (307511, 135)\n",
      "  > Test:  (48744, 134)\n",
      "\n",
      "‚úÖ Target Encoding and name cleaning complete. Final Feature Count:\n",
      "  > Train Shape: (307511, 135)\n",
      "  > Test Shape:  (48744, 134)\n",
      "‚úÖ MLOps Artifact: Final TargetEncoder saved to: /content/drive/MyDrive/Project_01/models/final_target_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Target Encoding and Final Cleanup\n",
    "\n",
    "print(\"--- Starting: K-Fold Target Encoding (Leakage-safe) ---\")\n",
    "\n",
    "# 1. Identify categorical features (raw, before encoding)\n",
    "categorical_features = df_train_final.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "TARGET_COLUMN = 'TARGET'\n",
    "\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Optional: save categorical features list as an artifact\n",
    "categorical_features_df = pd.DataFrame(categorical_features, columns=['cat_feat'])\n",
    "categorical_features_path = os.path.join(cfg.REPORT_DIR, 'categorical_features.csv')\n",
    "categorical_features_df.to_csv(categorical_features_path, index=False)\n",
    "print(f\"‚úÖ Categorical features list saved to: {categorical_features_path}\")\n",
    "\n",
    "# 2. Out-of-Fold Target Encoding for the TRAIN set\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# DataFrame to hold OOF encoded values (same index as df_train_final)\n",
    "oof_encoded = pd.DataFrame(index=df_train_final.index, columns=categorical_features, dtype=float)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_train_final), start=1):\n",
    "    print(f\"  -> Fitting TargetEncoder on fold {fold}/{N_FOLDS}...\")\n",
    "    \n",
    "    te_fold = TargetEncoder(cols=categorical_features, smoothing=0.3)\n",
    "    te_fold.fit(\n",
    "        df_train_final.loc[train_idx, categorical_features],\n",
    "        df_train_final.loc[train_idx, TARGET_COLUMN]\n",
    "    )\n",
    "    \n",
    "    # Transform validation fold and store only encoded columns\n",
    "    encoded_val = te_fold.transform(df_train_final.loc[val_idx, categorical_features])\n",
    "    # Keep only the encoded categorical columns\n",
    "    encoded_val = encoded_val[categorical_features]\n",
    "    \n",
    "    oof_encoded.iloc[val_idx] = encoded_val.values\n",
    "\n",
    "# 3. Fit a FINAL TargetEncoder on the FULL TRAIN set (for TEST + INFERENCE)\n",
    "print(\"\\n--- Fitting final TargetEncoder on the full training set (for test + API) ---\")\n",
    "final_target_encoder = TargetEncoder(cols=categorical_features, smoothing=0.3)\n",
    "final_target_encoder.fit(\n",
    "    df_train_final[categorical_features],\n",
    "    df_train_final[TARGET_COLUMN]\n",
    ")\n",
    "\n",
    "# Handle any remaining NaNs in OOF encoded data (e.g., if some rows were not covered properly)\n",
    "encoded_full_train = final_target_encoder.transform(df_train_final[categorical_features])[categorical_features]\n",
    "oof_encoded = oof_encoded.fillna(encoded_full_train)\n",
    "\n",
    "# Rename encoded columns with a clear suffix\n",
    "oof_encoded.columns = [f\"{col}_TARGET_ENC\" for col in categorical_features]\n",
    "\n",
    "print(f\"‚úÖ OOF Target Encoding completed. Encoded TRAIN shape: {oof_encoded.shape}\")\n",
    "\n",
    "# 4. Apply TargetEncoder to the TEST set using the final encoder\n",
    "encoded_test = final_target_encoder.transform(df_test_final[categorical_features])[categorical_features]\n",
    "encoded_test.columns = [f\"{col}_TARGET_ENC\" for col in categorical_features]\n",
    "\n",
    "print(f\"‚úÖ TEST Target Encoding completed. Encoded TEST shape: {encoded_test.shape}\")\n",
    "\n",
    "# 5. Drop raw categorical features and append encoded ones (TRAIN and TEST)\n",
    "df_train_final = pd.concat(\n",
    "    [\n",
    "        df_train_final.drop(columns=categorical_features, errors='ignore'),\n",
    "        oof_encoded\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_test_final = pd.concat(\n",
    "    [\n",
    "        df_test_final.drop(columns=categorical_features, errors='ignore'),\n",
    "        encoded_test\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nShapes after adding encoded features and dropping raw categoricals:\")\n",
    "print(f\"  > Train: {df_train_final.shape}\")\n",
    "print(f\"  > Test:  {df_test_final.shape}\")\n",
    "\n",
    "# 6. Clean column names (remove special characters) ‚Äì same function used before\n",
    "def clean_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = df.columns\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        new_col = re.sub(r'[^A-Za-z0-9_]+', '', col)\n",
    "        new_cols.append(new_col)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "df_train_final = clean_names(df_train_final)\n",
    "df_test_final = clean_names(df_test_final)\n",
    "\n",
    "print(f\"\\n‚úÖ Target Encoding and name cleaning complete. Final Feature Count:\")\n",
    "print(f\"  > Train Shape: {df_train_final.shape}\")\n",
    "print(f\"  > Test Shape:  {df_test_final.shape}\")\n",
    "\n",
    "# 7. Save FINAL TargetEncoder as MLOps artifact (used later by the API)\n",
    "encoder_path = os.path.join(cfg.MODEL_DIR, 'final_target_encoder.pkl')\n",
    "joblib.dump(final_target_encoder, encoder_path)\n",
    "print(f\"‚úÖ MLOps Artifact: Final TargetEncoder saved to: {encoder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa10bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Final Processed DataFrames (Parquet Format) ---\n",
      "‚úÖ Final Train set saved to: /content/drive/MyDrive/Project_01/data/processed/train_final_encoded.parquet\n",
      "‚úÖ Final Test set saved to: /content/drive/MyDrive/Project_01/data/processed/test_final_encoded.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Saving Final Processed DataFrames (Parquet Format) ---\")\n",
    "\n",
    "# Define the save paths using the 'cfg' instance (assuming cfg.DATA_PROCESSED_DIR exists)\n",
    "FINAL_TRAIN_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'train_final_encoded.parquet')\n",
    "FINAL_TEST_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'test_final_encoded.parquet')\n",
    "\n",
    "\n",
    "# 1. Save the Training DataFrame\n",
    "# Use index=False as 'TIME_INDEX' should now be a regular column after Block 13\n",
    "df_train_final.to_parquet(FINAL_TRAIN_FILE, index=False)\n",
    "print(f\"‚úÖ Final Train set saved to: {FINAL_TRAIN_FILE}\")\n",
    "\n",
    "# 2. Save the Test DataFrame\n",
    "df_test_final.to_parquet(FINAL_TEST_FILE, index=False)\n",
    "print(f\"‚úÖ Final Test set saved to: {FINAL_TEST_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
