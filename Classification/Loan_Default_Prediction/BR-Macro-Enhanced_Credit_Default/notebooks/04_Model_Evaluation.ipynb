{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0728a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras) (3.10.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scikeras) (1.6.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (3.15.1)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.18.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.5.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=3.2.0->scikeras) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# scikeras not itstalled in the colab environment\n",
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6acdb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CORE LIBRARIES AND UTILITIES ---\n",
    "# Standard data manipulation and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# --- 2. SCIKIT-LEARN: MODEL SELECTION & METRICS ---\n",
    "# Tools for splitting data, cross-validation, and performance evaluation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    KFold, \n",
    "    cross_val_score, \n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline # For creating robust modeling workflows\n",
    "from sklearn.impute import SimpleImputer # Tool to handle NaNs\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "# --- 3. SCIKIT-LEARN: BASE CLASSIFIERS ---\n",
    "# Linear Models, Instance-Based, and Probabilistic Models\n",
    "from sklearn.linear_model import LogisticRegression # Linear Model\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # Discriminant Model\n",
    "from sklearn.naive_bayes import GaussianNB # Probabilistic Model\n",
    "from sklearn.neighbors import KNeighborsClassifier # Instance-based Model\n",
    "from sklearn.tree import DecisionTreeClassifier # Tree-based Model\n",
    "\n",
    "# Note: SVC (Support Vector Machine) is often excluded from rapid exploration \n",
    "# due to its high computational cost on large datasets.\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neural_network import MLPClassifier # Simple Neural Network\n",
    "\n",
    "# --- 4. SCIKIT-LEARN: ENSEMBLE MODELS ---\n",
    "# Advanced classifiers for improved performance (often the top performers)\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, \n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier, \n",
    "    ExtraTreesClassifier\n",
    ")\n",
    "# modern boosting libraries that often outperform Scikit-learn ensembles:\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# --- 5. DEEP LEARNING (Keras/TensorFlow) ---\n",
    "# Note: Using SciKeras for Scikit-learn compatibility is the modern best practice.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout # Added Dropout for robustness\n",
    "from scikeras.wrappers import KerasClassifier # Modern replacement\n",
    "from tensorflow.keras.optimizers import SGD, Adam # Adam is usually preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "388bc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1bbd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project Path in Colab\n",
    "PROJECT_BASE_PATH = '/content/drive/MyDrive/Project_01' \n",
    "\n",
    "# ADD 'src' DIRECTORY TO PYTHON PATH\n",
    "SRC_PATH = os.path.join(PROJECT_BASE_PATH, 'src')\n",
    "\n",
    "# verify if SRC_PATH is already in sys.path\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "    print(\"✅ Successfully added 'src' directory to Python path.\")\n",
    "\n",
    "# IMPORT Paths CLASS FROM config MODULE\n",
    "from config import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98ce8742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Project configuration (Paths) initialized successfully.\n",
      "Raw Data Path check: /content/drive/MyDrive/Project_01/data/raw/application_train.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from config import Paths\n",
    "    \n",
    "    # 3. Inicialize a instância com um nome único (cfg)\n",
    "    cfg = Paths(PROJECT_BASE_PATH) # <-- Mudança aqui\n",
    "    cfg.create_dirs() \n",
    "    \n",
    "    print(\"\\n✅ Project configuration (Paths) initialized successfully.\")\n",
    "    print(f\"Raw Data Path check: {cfg.TRAIN_RAW_FILE}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"❌ Error: Could not import Paths from config module.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0186bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Encoded Train Data. Shape: (307511, 135)\n",
      "✅ Loaded Encoded Test Data. Shape: (48744, 134)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FINAL_TRAIN_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'train_final_encoded.parquet')\n",
    "FINAL_TEST_FILE = os.path.join(cfg.DATA_PROCESSED_DIR, 'test_final_encoded.parquet')\n",
    "\n",
    "try:\n",
    "    # Read the Parquet files\n",
    "    df_train_final = pd.read_parquet(FINAL_TRAIN_FILE)\n",
    "    df_test_final = pd.read_parquet(FINAL_TEST_FILE)\n",
    "    \n",
    "    print(f\"✅ Loaded Encoded Train Data. Shape: {df_train_final.shape}\")\n",
    "    print(f\"✅ Loaded Encoded Test Data. Shape: {df_test_final.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ ERROR: Parquet files not found. Ensure Block 13 was executed successfully and saved the files.\")\n",
    "    # Exit or raise an error if critical data is missing\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1e0112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Comprehensive Model Cross-Validation on SIMPLIFIED Data ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running 3-Fold Cross-Validation for 6 models ---\n",
      "--------------------------------------------------\n",
      "Model: Logistic Regression (Scaled)\n",
      "Mean AUC: 0.7459 (Std Dev: 0.0021)\n",
      "Training Time: 55.63 seconds\n",
      "--------------------------------------------------\n",
      "Model: K-Nearest Neighbors (Scaled)\n",
      "Mean AUC: 0.5748 (Std Dev: 0.0003)\n",
      "Training Time: 876.44 seconds\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree (Clean)\n",
      "Mean AUC: 0.5395 (Std Dev: 0.0025)\n",
      "Training Time: 99.57 seconds\n",
      "--------------------------------------------------\n",
      "Model: Random Forest (Clean)\n",
      "Mean AUC: 0.7264 (Std Dev: 0.0018)\n",
      "Training Time: 111.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model: LightGBM (Base)\n",
      "Mean AUC: 0.7586 (Std Dev: 0.0027)\n",
      "Training Time: 199.39 seconds\n",
      "--------------------------------------------------\n",
      "Model: XGBoost (Base)\n",
      "Mean AUC: 0.7211 (Std Dev: 0.0095)\n",
      "Training Time: 129.56 seconds\n",
      "\n",
      "--- Summary of Model Performance (Ranked by Mean AUC) ---\n",
      "                          Model  Mean_AUC  Std_Dev_AUC  Time_Seconds\n",
      "4               LightGBM (Base)  0.758578     0.002664    199.387240\n",
      "0  Logistic Regression (Scaled)  0.745944     0.002082     55.631243\n",
      "3         Random Forest (Clean)  0.726444     0.001812    111.135025\n",
      "5                XGBoost (Base)  0.721120     0.009476    129.557973\n",
      "1  K-Nearest Neighbors (Scaled)  0.574846     0.000316    876.444904\n",
      "2         Decision Tree (Clean)  0.539471     0.002540     99.566021\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Comprehensive Model Cross-Validation on SIMPLIFIED Data ---\")\n",
    "\n",
    "# 1. Prepare Features (X) and Target (y)\n",
    "EXCLUDED_COLS = ['SK_ID_CURR', 'TARGET'] \n",
    "features = [col for col in df_train_final.columns if col not in EXCLUDED_COLS]\n",
    "\n",
    "X = df_train_final[features]\n",
    "y = df_train_final['TARGET']\n",
    "\n",
    "# 2. Define Preprocessing Steps\n",
    "# The data is much cleaner now, so we only need SimpleImputer to handle residual NaNs (like DAYS_EMPLOYED anomaly).\n",
    "# NOTE: We can skip the FunctionTransformer for 'Inf' replacement, as Target Encoding is unlikely to generate Infs.\n",
    "\n",
    "universal_imputer = Pipeline([\n",
    "    # Use 'mean' strategy to handle residual NaNs from the simplified feature engineering\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "\n",
    "# 3. Model List (Applying Imputation and Scaling strategically)\n",
    "models = [\n",
    "    # A. Linear & Scaled Models (Need Imputer + Scaler)\n",
    "    ('Logistic Regression (Scaled)', Pipeline([\n",
    "        ('preprocessor', universal_imputer),\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('classifier', LogisticRegression(solver='liblinear', random_state=42, C=0.01, max_iter=200))\n",
    "    ])),\n",
    "    ('K-Nearest Neighbors (Scaled)', Pipeline([\n",
    "        ('preprocessor', universal_imputer),\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('classifier', KNeighborsClassifier(n_neighbors=5))\n",
    "    ])),\n",
    "    \n",
    "    # B. Tree Models (Need Imputer for safety/consistency with base scikit-learn)\n",
    "    ('Decision Tree (Clean)', Pipeline([\n",
    "        ('preprocessor', universal_imputer),\n",
    "        ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "    ])),\n",
    "    ('Random Forest (Clean)', Pipeline([\n",
    "        ('preprocessor', universal_imputer),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1))\n",
    "    ])),\n",
    "\n",
    "    # C. Boosting Models (Imputer kept for consistency, although they handle NaNs)\n",
    "    ('LightGBM (Base)', Pipeline([\n",
    "        ('preprocessor', universal_imputer),\n",
    "        ('classifier', lgb.LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1, n_estimators=500))\n",
    "    ])),\n",
    "    ('XGBoost (Base)', Pipeline([\n",
    "        ('preprocessor', universal_imputer),\n",
    "        ('classifier', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1, n_estimators=500))\n",
    "    ])),\n",
    "]\n",
    "\n",
    "# 4. Set up Cross-Validation strategy\n",
    "NFOLDS = 3 \n",
    "kfold = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "cv_results = []\n",
    "model_names = []\n",
    "training_times = []\n",
    "\n",
    "\n",
    "# 5. Training Loop with Cross-Validation\n",
    "\n",
    "print(f\"--- Running {NFOLDS}-Fold Cross-Validation for {len(models)} models ---\")\n",
    "\n",
    "for name, model in models:\n",
    "    start_time = time()\n",
    "    \n",
    "    try:\n",
    "        # Perform Cross-Validation using ROC AUC (Area Under the Curve)\n",
    "        scores = cross_val_score(model, X, y, cv=kfold, scoring='roc_auc', n_jobs=-1)\n",
    "        \n",
    "        end_time = time()\n",
    "        \n",
    "        # Append results\n",
    "        cv_results.append(scores)\n",
    "        model_names.append(name)\n",
    "        training_times.append(end_time - start_time)\n",
    "        \n",
    "        # Print interim results\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        print(f\"Model: {name}\")\n",
    "        print(f\"Mean AUC: {scores.mean():.4f} (Std Dev: {scores.std():.4f})\")\n",
    "        print(f\"Training Time: {(end_time - start_time):.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error encountered for model {name}: {e}\")\n",
    "        cv_results.append([np.nan] * NFOLDS)\n",
    "        model_names.append(name)\n",
    "        training_times.append(np.nan)\n",
    "\n",
    "\n",
    "# 6. Final Summary\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Mean_AUC': [np.mean(res) for res in cv_results],\n",
    "    'Std_Dev_AUC': [np.std(res) for res in cv_results],\n",
    "    'Time_Seconds': training_times\n",
    "}).sort_values(by='Mean_AUC', ascending=False)\n",
    "\n",
    "print(\"\\n--- Summary of Model Performance (Ranked by Mean AUC) ---\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ae6c6",
   "metadata": {},
   "source": [
    "LightTGBM has shown to be the best model, but it is important to notice that Logistic Regression Model also had a great perfomance in the scaled base. The running time for LightGBM is four times longer than Logistic Regression running time, and since LightGBM can deal better with non-linear relations and is trained in the base without scalation, I will perform a grid search for that algorithm in order to test some hyperparameters e maybe increase the Mean_AUC for LightGBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
